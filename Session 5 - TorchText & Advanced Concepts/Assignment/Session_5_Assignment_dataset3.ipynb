{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 5-Assignment_dataset3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishasathish13/TheSchoolofAI-END3.0/blob/main/Session%205%20-%20TorchText%20%26%20Advanced%20Concepts/Assignment/Session_5_Assignment_dataset3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VaVIQPawdbU"
      },
      "source": [
        "from torchtext.datasets import YahooAnswers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIDrrV3AcpNu",
        "outputId": "b536b870-403d-48dd-c812-b9bd88545bdb"
      },
      "source": [
        "help(YahooAnswers)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function YahooAnswers in module torchtext.datasets.yahooanswers:\n",
            "\n",
            "YahooAnswers(root='.data', split=('train', 'test'))\n",
            "    YahooAnswers dataset\n",
            "    \n",
            "    Separately returns the train/test split\n",
            "    \n",
            "    Number of lines per split:\n",
            "        train: 1400000\n",
            "    \n",
            "        test: 60000\n",
            "    \n",
            "    \n",
            "    Number of classes\n",
            "        10\n",
            "    \n",
            "    \n",
            "    Args:\n",
            "        root: Directory where the datasets are saved.\n",
            "            Default: .data\n",
            "        split: split or splits to be returned. Can be a string or tuple of strings.\n",
            "            Default: ('train', 'test')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBvZyNEAwtgf",
        "outputId": "bad613cf-ae0e-47b4-e7f6-ec5ea2b0cda5"
      },
      "source": [
        "train_iter = YahooAnswers(split='train')\n",
        "next(train_iter)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 319M/319M [00:02<00:00, 145MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5,\n",
              " \"why doesn't an optical mouse work on a glass table? or even on some surfaces? Optical mice use an LED and a camera to rapidly capture images of the surface beneath the mouse.  The infomation from the camera is analyzed by a DSP (Digital Signal Processor) and used to detect imperfections in the underlying surface and determine motion. Some materials, such as glass, mirrors or other very shiny, uniform surfaces interfere with the ability of the DSP to accurately analyze the surface beneath the mouse.  \\\\nSince glass is transparent and very uniform, the mouse is unable to pick up enough imperfections in the underlying surface to determine motion.  Mirrored surfaces are also a problem, since they constantly reflect back the same image, causing the DSP not to recognize motion properly. When the system is unable to see surface changes associated with movement, the mouse will not work properly.\")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrC7Ywp0w2j9",
        "outputId": "6afcc542-9017-4b31-8a6f-9fe4bd5e8572"
      },
      "source": [
        "# or iterate over a loop\n",
        "\n",
        "#to display only the first 20 lines\n",
        "for (ln, (label, line)) in enumerate(train_iter):\n",
        "  print(ln, label, line)\n",
        "  if ln == 20:\n",
        "    break\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 6 What is the best off-road motorcycle trail ? long-distance trail throughout CA i hear that the mojave road is amazing!<br />\\nsearch for it online.\n",
            "1 3 What is Trans Fat? How to reduce that? I heard that tras fat is bad for the body.  Why is that? Where can we find it in our daily food? Trans fats occur in manufactured foods during the process of partial hydrogenation, when hydrogen gas is bubbled through vegetable oil to increase shelf life and stabilize the original polyunsatured oil. The resulting fat is similar to saturated fat, which raises \"bad\" LDL cholesterol and can lead to clogged arteries and heart disease. \\nUntil very recently, food labels were not required to list trans fats, and this health risk remained hidden to consumers. In early July, FDA regulations changed, and food labels will soon begin identifying trans fat content in processed foods.\n",
            "2 7 How many planes Fedex has? I heard that it is the largest airline in the world according to the www.fedex.com web site:\\nAir Fleet<br />\\n  <br />\\n670 aircraft, including: <br />\\n47 Airbus A300-600s  17 Boeing DC10-30s  <br />\\n62 Airbus A310-200/300s  36 Boeing MD10-10s  <br />\\n2 ATR 72s  5 Boeing MD10-30s  <br />\\n29 ATR 42s  57 Boeing MD11s  <br />\\n18 Boeing 727-100s  10 Cessna 208As  <br />\\n94 Boeing 727-200s  246 Cessna 208Bs  <br />\\n30 Boeing DC10-10s  17 Fokker F-27s\n",
            "3 7 In the san francisco bay area, does it make sense to rent or buy ? the prices of rent and the price of buying does not make sense to me, mostly the rent will not cover the mortgage . Is it better to rent a house or to buy? renting vs buying depends on your goals. <br />\\ngenerally thinking is that buying is better b/c the payments that would go into the rent start building equity in your home. the govt also incentivizes you to buy by making your property tax payments and mortgage interest payments tax deductible.\\nhaving said that current housing status in the bay area is such that housing cost to purchase is relatively high and rental prices (compared to ownership cost) are relatively low (relative to the rest of the country). it makes lese sense to buy vs. other places.\\nbottom line you should base your decision on whether you think the market will keep going up or not. the other numbers tend to even out, the main gain or loss in buying comes from appreciation/depreciation.\n",
            "4 5 What's the best way to clean a keyboard? I have very small stuff stuck under my keyboard and it prevents it to from working. What will be the be\\nst way to clean it? There are commercial kits available, but a can of compressed air, a lint-free cloth or wipes, mild dishwashing liquid, and a vacuum cleaner with a soft brush are all you really need for a basic cleaning. \\nAfter turning off your computer and unplugging your keyboard, gently shake the keyboard upside down over some newspaper to dislodge loose crumbs and particles. \\nUse the can of compressed air to blow a stream of air between the keys. \\nDampen a lint-free cloth with a diluted solution of dishwashing liquid and water to wipe down the keys. \\nUse the vacuum cleaner brush attachment to suck away any remaining dirt or debris. <br />\\nIf you have a membrane-type keyboard (it doesn't have a spring under each key) and the keys are truly grungy, you can remove the keys with a small screwdriver and use cotton swabs and 90% isopropyl alcohol to get it sparkling clean. If you are still feeling industrious, next you can tackle cleaning your mouse. \\nOf course, if cleaning your keyboard seems a truly undesirable task, you can always buy a new one for around $20.\n",
            "5 2 Why do people blush when they are embarrassed? Why do people blush when they are embarrassed? from ask yahoo...\\nhttp://ask.yahoo.com/ask/20040113.html\\n<<Blushing is a unique blend of evolutionary and social behavior. It's an involuntary reaction of the sympathetic nervous system, which is responsible for our \"fight or flight\" response, but blushing is solely triggered by social cues. <br />\\nPeople generally blush when they're feeling embarrassed, scared, or stressed. As a result of the \"fight or flight\" response, the capillaries that carry blood to the skin widen, and the increased blood flow lends the face, as well as sometimes the chest, neck, or even the body or legs, a reddened color. \\nExcessive facial blushing, or erythrophobia, is caused by overactivity of the sympathetic nervous system. The condition can cause a lot of psychological duress and has engendered several support groups. \\nIt's common knowledge that animals don't blush. So while there are some evolutionary cues behind blushing, it's also linked to something uniquely human -- moral consciousness.>>\n",
            "6 8 Is Lin Qingxia (aka Brigitte Lin) \"the most beautiful woman in Chinese cinema?\" This is according to Stephen Chow (http://www.hkentreview.com/2005/features/kfh/kfhprem.html). Is it true? Who is the best-looking male star? Did they make any movies together? Well.  Everyone has different definition on what 'beauty' is.  I like Lin Qingxia, but I think many girls are prettier than she was. (She is more than 40 years old now). \\nIf \"Lin Qingxia\" is the most beautiful woman in the Chinese cinema, the most handsome man in Chinese cinema should be \"Chin Han\" because they always made movies together.\\nHowever, A male movie star once was asked his girlfriend in real life or the girlfriend in movie is more beautiful.  He gave a very good answer: \"I think my mother is the most beautiful woman in the world.\"  :)\n",
            "7 5 What is the origin of \"foobar\"? I want to know the meaning of the word and how to explain to my friends. Not sure if this is the origin, but I think it was popularized in the 1989 film \"Tango & Cash\". After Tango, played by Sylvester Stallone, and Cash, portrayed by Kurt Russell, were thrown into jail and setup in a failed jailbreak attempt, the bad guy Yves Perret (Jack Pallance) locked them in the boiler room with a bunch of felons Tango & Cash had put in jail. Prior to unleashing a can of whoop-ass, their conversation was as follows:\\nGabriel Cash: I don't know about you, but I have an aversion to getting F.U.B.A.R...\\nRay Tango: What's F.U.B.A.R.?\\nGabriel Cash: Fucked-Up Beyond All Recognition.\n",
            "8 2 How the human species evolved? How the human species evolved? A tough question as it overlaps science and theology. Since you asked \"how the human species evolved?\" I'll assume you're interested in the scientific approach to the answer.\\nThe current theory holds that Homo sapiens evolved from Hominid ancestors over the course of millions of years through a process called natural selection. Natural selection is the weeding and advancement of species variants over time based on the fit in that species' then current environments. The pressures for \"fit\" are competitive (with each other, with other tribes, and with other animals) and environmental (weather, terrain, availability of food, etc.). These factors all contributed to the development of our higher brain, which we now use to evolve as a global collective through communication & technology.\n",
            "9 4 Who said the statement below and what does it mean? Can someone help me with understanding someone else's wisdom, please?&#xd;<br><br>\" It is very easy in the world to live by the opinion of the world. It is very easy in solitude to be self-centered. But the finished man is he who in the midst of the crowd keeps with perfect sweetness the independence of solitude. \" That is kind of a tricky little quote. Sometimes placing it in context can help. This is a quote from Ralph Waldo Emerson's essay \"Self Reliance,\" which discourages basing one's opinions and actions on common opinion in favor of self-trust (see the full essay at http://www.emersoncentral.com/selfreliance.htm). \\nI think the point of this quote is that it's very desireable to have one's own original thoughts and opinions, and to live by them.  It's easier to live by one's own convictions and not be persuaded by others when in isolation (that's the \"independence of solitude\"). However, not so special to isolate oneself from others in order to attain this ideal. Because it's difficult to live by one's own thoughts and opinions while being a part of society, it's something only a \"finished man\" can truly do. Also, in the essay, Emerson refers to being childlike - unselfish, non-judgmental, open-minded, uncynical, unhesitant - as something to admire. So I think that's where the seemingly misplaced \"perfect sweetness\" comes in. When a man can remain pure of heart while living in the harsh world and also keeping and living by his own ideals. \\nWhat do you think?\n",
            "10 4 How do I find an out of print book? When I was a kid I remember seeing a book that was like an yearbook of all newspapers published by the Times during WW II. Each of the years is compiled into a different book. &#xd;<br>It gave one a very uniqie perspecitev into the UK druing the war, and even had advertisements from thaat time.&#xd;<br>Anybody out there know how to track such books? There are several websites that you can find rare or out of print books.  A couple would be alibris.com or abebooks.com.  These sites list books by booksellers all over the country and some internationally.\n",
            "11 7 What are some tips on finding a good mortgage broker? What are some tips on finding a good mortgage broker? the most basic thing is to have someone you have a decent amount of trust for. someone recommended by a friend etc.\\nthe things you have to look for are:<br />\\n1. top priority is seeing that the broker can actually get your loan on time. many deals fall through b/c the broker cannot get the deal through on time and has less control than the original lender<br />\\n2. secondary priority is to see that they have access to competitive rates. not all brokers have access to all sources and rates, although most of them will be similar.<br />\\n3. you need to see that the broker is someone you trust to get you the best rate regardless of how much money they make off the transaction.\n",
            "12 5 what's the best way to create a bootable windos/dos CD? i don't use floppies any more and need to boot from something other than my hard disk Well, the best way is to look at whatever program you have for burning CDs and see if it has an option to create a bootable CD.  If you can't find it, or use Windows itself to burn CDs, then it's a little more complicated.\\nNote: If you find that booting from CD doesn't work, you may have to adjust your BIOS setting to allow your machine to boot from CD.\\nIf you want to boot to windows: the easiest way is probably to go here: http://www.nu2.nu/bootcd/ and download a utility that will do it for you.  There are instructions there depending on what type of boot you want.\\nIf you don't trust using a third party utility, Microsoft has some instructions here: http://support.microsoft.com/kb/167685/EN-US/ .  This process is not very straightforward though.\\nIf you just need to get into your filesystem and poke around, you might consider booting a different OS.  For example, http://www.freedos.org/ (FreeDOS) and http://www.knoppix.net/ (Knoppix Linux) may do what you want.  For these, you can download ISO images and burn bootable CDs.\n",
            "13 9 what is the reason for the increasing divorce percentage in the western world? what is the most common parameter that causing couples to separate IMO... our lives are much more complex than our equivalents from 50 years ago and thus it's more likely for married people to grow in different directions over time (values, needs, etc.). Add to that how easy and acceptable it is to get a divorce, and how quick we are to dispose of things that we no longer want, and you get an increasing percentage.\\nI wouldn't be surprised if in the not too distant future the average length of a marriage drops to ~5 years and the average number of marriages per person increases to 2+.\\nA parallel to consider: how many different circles of friends do you have and how often have you joined new ones and left others behind over your life? (grade school, high school, college, jobs, etc.). We are a nomadic species. :)\n",
            "14 2 What is an \"imaginary number\"? What is an \"imaginary number\", and how is it treated in algebra equations? Imaginary numbers are numbers than when squared equal a negative number, as in i^2 = -1, where i is the imaginary number. You'll also often see them represented as i = √-1 (that's the square root of -1).\\nDon't be confused by the poorly chosen name - imaginary numbers do indeed exist and are used in advanced math, such as in the physics of electromagnetic fields. The analogy that Wikipedia uses is a good one - just like you don't need the concept of fractions to count stones, it doesn't mean that fractions don't exist. :)\n",
            "15 2 Faxing a pizza would we be able to fax a pizza in the future? How far are we from creating a machine that can beam people from one place to another? We're pretty far away from being able to beam anything in the way that you are describing.  There are some experiments where splitting entangled electrons allows us to observe some interesting 'action at a distance' effects, but the amount of data that would need to be stored to transmit something with as many atoms as a pizza is incredible.  Also, such an operation would require nano-assemblers that could interpret such data an reassemble the pizza from the info provided.\n",
            "16 8 What are good sources to find out about new gospel artists? Is there a site that focuses primarily on gospel? CCM Magazine or their online website will give you information on up and coming artists in the Christian Music field.\n",
            "17 2 space missions Why are we spending so much money on space when we do not have a decent health insurance to all the public? You also need to consider the economic effects of funding basic research.  The moon missions of the 60's inspired an entire generation and helped to move many kids toward careers in science and engineering.  As a result, the US was well poised to take advantage of the huge boom in information technology in the last few decades.  The net effect of these engineers in the workforce could be see has a pretty impressive return on investment for the cost of those missions.\n",
            "18 2 How a black hole is formed? I would like to know how a black hole can possibly be formed. Are there any experimental evidence of such creation? The current scientific theory holds that black holes are formed when stars of sufficient mass (about 3x the mass of our Sun) reach the end of their life and collapse down into a singularity, which is, in essence, an infinitely small point with a huge amount of gravity - gravity so powerful that not even light can escape it's pull if it gets too close. It is also thought that black holes can be formed from smaller masses if external energy/pressure is exerted to squeeze the mass into a singularity.\\nNot sure if there's any direct experimental evidence of creation, but there is evidence that they exist. Cosmologists use several different methods to detect blacks holes, such as by looking for light being bent and/or disappearing as it travels from remote stars towards us, and also by looking for bursts of x-rays that are given off as matter is sucked into the back hole and destroyed.\\nFun fact: current evidence and theory suggests that the center of the Milky Way is a giant black hole that was formed as the densly packed stars in the center collapsed and sucked each other in.\n",
            "19 2 Heavy water what is the role that heavy water plays in the nuclear explosion process? Heavy water is like regular water except that instead of two hydrogen atoms and an oxygen atom (H20) it has two deuterium atoms and an oxygen atom (D20). Deuterium is a hydrogen atom with an extra neutron.\\nWater is used in nuclear reactors to moderate the reaction speed, and to absorb heat. Nuclear fission starts by giving off neutrons that create a chain reaction. Any water around it absorbs those neutrons (becoming heavy water) and slows the reaction. If heavy water is used instead of regular hydrogen water, the neutrons are not absorbed. This means that less pure nuclear material can be used to sustain a reaction. For example unrefined uranium, which is much less expensive, can be used instead of enriched uranium.\n",
            "20 8 When will Her Majesty be released in Dvd format? Her Majesty was a movie in limited release early in 2005.  I am interested in getting a copy of it in dvd format upon release. It's already available in Australia, if your DVD player can handle PAL:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDP3CqFKxZ7X",
        "outputId": "abf906e7-d5ac-445f-8f48-a09222810ae5"
      },
      "source": [
        "# dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "train_iter = YahooAnswers(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=16, shuffle=False)\n",
        "\n",
        "next(iter(dataloader))\n",
        "#displays 8 labels as batch size = 8"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([5, 6, 3, 7, 7, 5, 2, 8, 5, 2, 4, 4, 7, 5, 9, 2]),\n",
              " (\"why doesn't an optical mouse work on a glass table? or even on some surfaces? Optical mice use an LED and a camera to rapidly capture images of the surface beneath the mouse.  The infomation from the camera is analyzed by a DSP (Digital Signal Processor) and used to detect imperfections in the underlying surface and determine motion. Some materials, such as glass, mirrors or other very shiny, uniform surfaces interfere with the ability of the DSP to accurately analyze the surface beneath the mouse.  \\\\nSince glass is transparent and very uniform, the mouse is unable to pick up enough imperfections in the underlying surface to determine motion.  Mirrored surfaces are also a problem, since they constantly reflect back the same image, causing the DSP not to recognize motion properly. When the system is unable to see surface changes associated with movement, the mouse will not work properly.\",\n",
              "  'What is the best off-road motorcycle trail ? long-distance trail throughout CA i hear that the mojave road is amazing!<br />\\\\nsearch for it online.',\n",
              "  'What is Trans Fat? How to reduce that? I heard that tras fat is bad for the body.  Why is that? Where can we find it in our daily food? Trans fats occur in manufactured foods during the process of partial hydrogenation, when hydrogen gas is bubbled through vegetable oil to increase shelf life and stabilize the original polyunsatured oil. The resulting fat is similar to saturated fat, which raises \"bad\" LDL cholesterol and can lead to clogged arteries and heart disease. \\\\nUntil very recently, food labels were not required to list trans fats, and this health risk remained hidden to consumers. In early July, FDA regulations changed, and food labels will soon begin identifying trans fat content in processed foods.',\n",
              "  'How many planes Fedex has? I heard that it is the largest airline in the world according to the www.fedex.com web site:\\\\nAir Fleet<br />\\\\n  <br />\\\\n670 aircraft, including: <br />\\\\n47 Airbus A300-600s  17 Boeing DC10-30s  <br />\\\\n62 Airbus A310-200/300s  36 Boeing MD10-10s  <br />\\\\n2 ATR 72s  5 Boeing MD10-30s  <br />\\\\n29 ATR 42s  57 Boeing MD11s  <br />\\\\n18 Boeing 727-100s  10 Cessna 208As  <br />\\\\n94 Boeing 727-200s  246 Cessna 208Bs  <br />\\\\n30 Boeing DC10-10s  17 Fokker F-27s',\n",
              "  'In the san francisco bay area, does it make sense to rent or buy ? the prices of rent and the price of buying does not make sense to me, mostly the rent will not cover the mortgage . Is it better to rent a house or to buy? renting vs buying depends on your goals. <br />\\\\ngenerally thinking is that buying is better b/c the payments that would go into the rent start building equity in your home. the govt also incentivizes you to buy by making your property tax payments and mortgage interest payments tax deductible.\\\\nhaving said that current housing status in the bay area is such that housing cost to purchase is relatively high and rental prices (compared to ownership cost) are relatively low (relative to the rest of the country). it makes lese sense to buy vs. other places.\\\\nbottom line you should base your decision on whether you think the market will keep going up or not. the other numbers tend to even out, the main gain or loss in buying comes from appreciation/depreciation.',\n",
              "  \"What's the best way to clean a keyboard? I have very small stuff stuck under my keyboard and it prevents it to from working. What will be the be\\\\nst way to clean it? There are commercial kits available, but a can of compressed air, a lint-free cloth or wipes, mild dishwashing liquid, and a vacuum cleaner with a soft brush are all you really need for a basic cleaning. \\\\nAfter turning off your computer and unplugging your keyboard, gently shake the keyboard upside down over some newspaper to dislodge loose crumbs and particles. \\\\nUse the can of compressed air to blow a stream of air between the keys. \\\\nDampen a lint-free cloth with a diluted solution of dishwashing liquid and water to wipe down the keys. \\\\nUse the vacuum cleaner brush attachment to suck away any remaining dirt or debris. <br />\\\\nIf you have a membrane-type keyboard (it doesn't have a spring under each key) and the keys are truly grungy, you can remove the keys with a small screwdriver and use cotton swabs and 90% isopropyl alcohol to get it sparkling clean. If you are still feeling industrious, next you can tackle cleaning your mouse. \\\\nOf course, if cleaning your keyboard seems a truly undesirable task, you can always buy a new one for around $20.\",\n",
              "  'Why do people blush when they are embarrassed? Why do people blush when they are embarrassed? from ask yahoo...\\\\nhttp://ask.yahoo.com/ask/20040113.html\\\\n<<Blushing is a unique blend of evolutionary and social behavior. It\\'s an involuntary reaction of the sympathetic nervous system, which is responsible for our \"fight or flight\" response, but blushing is solely triggered by social cues. <br />\\\\nPeople generally blush when they\\'re feeling embarrassed, scared, or stressed. As a result of the \"fight or flight\" response, the capillaries that carry blood to the skin widen, and the increased blood flow lends the face, as well as sometimes the chest, neck, or even the body or legs, a reddened color. \\\\nExcessive facial blushing, or erythrophobia, is caused by overactivity of the sympathetic nervous system. The condition can cause a lot of psychological duress and has engendered several support groups. \\\\nIt\\'s common knowledge that animals don\\'t blush. So while there are some evolutionary cues behind blushing, it\\'s also linked to something uniquely human -- moral consciousness.>>',\n",
              "  'Is Lin Qingxia (aka Brigitte Lin) \"the most beautiful woman in Chinese cinema?\" This is according to Stephen Chow (http://www.hkentreview.com/2005/features/kfh/kfhprem.html). Is it true? Who is the best-looking male star? Did they make any movies together? Well.  Everyone has different definition on what \\'beauty\\' is.  I like Lin Qingxia, but I think many girls are prettier than she was. (She is more than 40 years old now). \\\\nIf \"Lin Qingxia\" is the most beautiful woman in the Chinese cinema, the most handsome man in Chinese cinema should be \"Chin Han\" because they always made movies together.\\\\nHowever, A male movie star once was asked his girlfriend in real life or the girlfriend in movie is more beautiful.  He gave a very good answer: \"I think my mother is the most beautiful woman in the world.\"  :)',\n",
              "  'What is the origin of \"foobar\"? I want to know the meaning of the word and how to explain to my friends. Not sure if this is the origin, but I think it was popularized in the 1989 film \"Tango & Cash\". After Tango, played by Sylvester Stallone, and Cash, portrayed by Kurt Russell, were thrown into jail and setup in a failed jailbreak attempt, the bad guy Yves Perret (Jack Pallance) locked them in the boiler room with a bunch of felons Tango & Cash had put in jail. Prior to unleashing a can of whoop-ass, their conversation was as follows:\\\\nGabriel Cash: I don\\'t know about you, but I have an aversion to getting F.U.B.A.R...\\\\nRay Tango: What\\'s F.U.B.A.R.?\\\\nGabriel Cash: Fucked-Up Beyond All Recognition.',\n",
              "  'How the human species evolved? How the human species evolved? A tough question as it overlaps science and theology. Since you asked \"how the human species evolved?\" I\\'ll assume you\\'re interested in the scientific approach to the answer.\\\\nThe current theory holds that Homo sapiens evolved from Hominid ancestors over the course of millions of years through a process called natural selection. Natural selection is the weeding and advancement of species variants over time based on the fit in that species\\' then current environments. The pressures for \"fit\" are competitive (with each other, with other tribes, and with other animals) and environmental (weather, terrain, availability of food, etc.). These factors all contributed to the development of our higher brain, which we now use to evolve as a global collective through communication & technology.',\n",
              "  'Who said the statement below and what does it mean? Can someone help me with understanding someone else\\'s wisdom, please?&#xd;<br><br>\" It is very easy in the world to live by the opinion of the world. It is very easy in solitude to be self-centered. But the finished man is he who in the midst of the crowd keeps with perfect sweetness the independence of solitude. \" That is kind of a tricky little quote. Sometimes placing it in context can help. This is a quote from Ralph Waldo Emerson\\'s essay \"Self Reliance,\" which discourages basing one\\'s opinions and actions on common opinion in favor of self-trust (see the full essay at http://www.emersoncentral.com/selfreliance.htm). \\\\nI think the point of this quote is that it\\'s very desireable to have one\\'s own original thoughts and opinions, and to live by them.  It\\'s easier to live by one\\'s own convictions and not be persuaded by others when in isolation (that\\'s the \"independence of solitude\"). However, not so special to isolate oneself from others in order to attain this ideal. Because it\\'s difficult to live by one\\'s own thoughts and opinions while being a part of society, it\\'s something only a \"finished man\" can truly do. Also, in the essay, Emerson refers to being childlike - unselfish, non-judgmental, open-minded, uncynical, unhesitant - as something to admire. So I think that\\'s where the seemingly misplaced \"perfect sweetness\" comes in. When a man can remain pure of heart while living in the harsh world and also keeping and living by his own ideals. \\\\nWhat do you think?',\n",
              "  'How do I find an out of print book? When I was a kid I remember seeing a book that was like an yearbook of all newspapers published by the Times during WW II. Each of the years is compiled into a different book. &#xd;<br>It gave one a very uniqie perspecitev into the UK druing the war, and even had advertisements from thaat time.&#xd;<br>Anybody out there know how to track such books? There are several websites that you can find rare or out of print books.  A couple would be alibris.com or abebooks.com.  These sites list books by booksellers all over the country and some internationally.',\n",
              "  'What are some tips on finding a good mortgage broker? What are some tips on finding a good mortgage broker? the most basic thing is to have someone you have a decent amount of trust for. someone recommended by a friend etc.\\\\nthe things you have to look for are:<br />\\\\n1. top priority is seeing that the broker can actually get your loan on time. many deals fall through b/c the broker cannot get the deal through on time and has less control than the original lender<br />\\\\n2. secondary priority is to see that they have access to competitive rates. not all brokers have access to all sources and rates, although most of them will be similar.<br />\\\\n3. you need to see that the broker is someone you trust to get you the best rate regardless of how much money they make off the transaction.',\n",
              "  \"what's the best way to create a bootable windos/dos CD? i don't use floppies any more and need to boot from something other than my hard disk Well, the best way is to look at whatever program you have for burning CDs and see if it has an option to create a bootable CD.  If you can't find it, or use Windows itself to burn CDs, then it's a little more complicated.\\\\nNote: If you find that booting from CD doesn't work, you may have to adjust your BIOS setting to allow your machine to boot from CD.\\\\nIf you want to boot to windows: the easiest way is probably to go here: http://www.nu2.nu/bootcd/ and download a utility that will do it for you.  There are instructions there depending on what type of boot you want.\\\\nIf you don't trust using a third party utility, Microsoft has some instructions here: http://support.microsoft.com/kb/167685/EN-US/ .  This process is not very straightforward though.\\\\nIf you just need to get into your filesystem and poke around, you might consider booting a different OS.  For example, http://www.freedos.org/ (FreeDOS) and http://www.knoppix.net/ (Knoppix Linux) may do what you want.  For these, you can download ISO images and burn bootable CDs.\",\n",
              "  \"what is the reason for the increasing divorce percentage in the western world? what is the most common parameter that causing couples to separate IMO... our lives are much more complex than our equivalents from 50 years ago and thus it's more likely for married people to grow in different directions over time (values, needs, etc.). Add to that how easy and acceptable it is to get a divorce, and how quick we are to dispose of things that we no longer want, and you get an increasing percentage.\\\\nI wouldn't be surprised if in the not too distant future the average length of a marriage drops to ~5 years and the average number of marriages per person increases to 2+.\\\\nA parallel to consider: how many different circles of friends do you have and how often have you joined new ones and left others behind over your life? (grade school, high school, college, jobs, etc.). We are a nomadic species. :)\",\n",
              "  'What is an \"imaginary number\"? What is an \"imaginary number\", and how is it treated in algebra equations? Imaginary numbers are numbers than when squared equal a negative number, as in i^2 = -1, where i is the imaginary number. You\\'ll also often see them represented as i = √-1 (that\\'s the square root of -1).\\\\nDon\\'t be confused by the poorly chosen name - imaginary numbers do indeed exist and are used in advanced math, such as in the physics of electromagnetic fields. The analogy that Wikipedia uses is a good one - just like you don\\'t need the concept of fractions to count stones, it doesn\\'t mean that fractions don\\'t exist. :)')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruhdtQ7JeUDK"
      },
      "source": [
        "help(DataLoader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OGbN_YIyHfG"
      },
      "source": [
        "Torchtext has revamped the very basic components of the torchtext library, including vocab, word vectors, tokenizer. These are the basic data processing building blocks for a raw text strings. \n",
        "\n",
        "Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Here we use built in factory function build_vocab_from_iterator which accepts iterator that yield list or iterator of tokens. Users can also pass any special symbols to be added to the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ITqjdsvxrnB"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = YahooAnswers(split='train')\n",
        "\n",
        "#to feed in sentences one by one\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "\n",
        "#sets the default index = 0 for the unknown words, i.e., words not in the vocabulary\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWej-eJ7R70m"
      },
      "source": [
        "def st(*transforms):\n",
        "  def func(txt_io):\n",
        "    for transform in transforms:\n",
        "      txt_io = transform(txt_io)\n",
        "    return txt_io\n",
        "  return func"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKhLvaKLzQek",
        "outputId": "a930bc98-e225-4395-fdd7-049f7909cbb1"
      },
      "source": [
        "#theschoolofai is the unknown word in our vocab, hence index set to 0\n",
        "vocab(['here', 'is', 'an', 'example', 'theschoolofai'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[51, 14, 68, 2030, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEg8TR080r_-"
      },
      "source": [
        "Prepare the text processing pipeline with the tokenizer and vocabulary. The text and label pipelines will be used to process the raw data strings from the dataset iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPIlKV9P0gi3"
      },
      "source": [
        "#we need to pass our sentences through the tokenizer first to normalise it before sending it the vocab\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x)-1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1fpaYln0zZH",
        "outputId": "4082497b-a750-4a36-933b-5790a7d9b9c6"
      },
      "source": [
        "text_pipeline('here is the an example')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[150, 11, 2, 55, 354]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLAt2Kyi01cZ",
        "outputId": "208a063a-ba50-4dfe-823e-a0bc9d0977c9"
      },
      "source": [
        "label_pipeline('10')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcLXuyuD1T4X"
      },
      "source": [
        "## Generate data batch and iterator\n",
        "\n",
        "torch.utils.data.DataLoader is recommended for PyTorch users. It works with a map-style dataset that implements the getitem() and len() protocols, and represents a map from indices/keys to data samples. It also works with an iterable dataset with the shuffle argument of False.\n",
        "\n",
        "Before sending to the model, collate_fn function works on a batch of samples generated from DataLoader. The input to collate_fn is a batch of data with the batch size in DataLoader, and collate_fn processes them according to the data processing pipelines declared previously. Pay attention here and make sure that collate_fn is declared as a top level def. This ensures that the function is available in each worker."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbhW4lz606fh"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpays4p95I_z"
      },
      "source": [
        "EmbeddingBag is a useful feature to consume sparse ids and produce embeddings. Later we are going to see examples where the text entries in the original batch input will be backed into a list and concatenated as a single tensor for the input of nn.EmbeddingBag.\n",
        "\n",
        "It is doing two things. The first step is to create an embedding and the second step is to reduce (sum/mean/max, according to the \"mode\" argument) the embedding output across dimension 0. So this is equivalent to torch.nn.functional.embedding, followed by torch.sum/mean/max. However, the conceptual two step process does not reflect how it's actually implemented. Since embedding_bag does not need to return the intermediate result, it doesn't actually generate a Tensor object for the embedding. It just goes straight to computing the reduction, pulling in the appropriate data from the weight argument according to the indices in the input argument. Avoiding the creation of the embedding Tensor allows for better performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmBi1Nlb2lXr",
        "outputId": "cde7a4a5-78f7-4c39-a1b9-5a7d30795162"
      },
      "source": [
        "weight = torch.randn(3, 4)\n",
        "weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1543, -1.5977, -0.2257,  0.1658],\n",
              "        [-0.3355, -2.6362,  0.4767, -0.1704],\n",
              "        [ 2.6389,  0.8871, -1.5911,  0.5172]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1OmVDGp5N_f",
        "outputId": "e555717b-ec0c-4d7b-f913-b98b7e9cdafc"
      },
      "source": [
        "indices = torch.tensor([2, 1])\n",
        "indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmI3jesm5UEh",
        "outputId": "dc8eae99-3c37-4239-94af-14c7863777e6"
      },
      "source": [
        "embedding_dwork = torch.nn.functional.embedding(indices, weight)\n",
        "embedding_dwork"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.6389,  0.8871, -1.5911,  0.5172],\n",
              "        [-0.3355, -2.6362,  0.4767, -0.1704]])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLPLNGmj5ehJ",
        "outputId": "1c3f8359-ea12-4d9d-83d7-d738a1ba63ba"
      },
      "source": [
        "embedding_dwork_mean = embedding_dwork.mean(dim=0, keepdim=True)\n",
        "embedding_dwork_mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.1517, -0.8746, -0.5572,  0.1734]])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86xXxo575mlx",
        "outputId": "75a92117-ee83-4443-86f0-43eca7ec9210"
      },
      "source": [
        "embedding_bag = torch.nn.functional.embedding_bag(indices, weight, torch.tensor([0]), mode='mean')\n",
        "embedding_bag"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.1517, -0.8746, -0.5572,  0.1734]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG9ExKRG6z2M"
      },
      "source": [
        "In this example, the text entries in the original data batch input are packed into a list and concatenated as a single tensor for the input of nn.EmbeddingBag. The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fo_Yp1I5tQ_"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label)) #appending the labels to label_list\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text) \n",
        "         offsets.append(processed_text.size(0)) #appending the length of the sentence\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)    "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQYlLbzq629a"
      },
      "source": [
        "train_iter = YahooAnswers(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=16, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy1IdVAv7Xod"
      },
      "source": [
        "The model is composed of the nn.EmbeddingBag <https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag>__ layer plus a linear layer for the classification purpose. nn.EmbeddingBag with the default mode of \"mean\" computes the mean value of a “bag” of embeddings. Although the text entries here have different lengths, nn.EmbeddingBag module requires no padding here since the text lengths are saved in offsets.\n",
        "\n",
        "Additionally, since nn.EmbeddingBag accumulates the average across the embeddings on the fly, nn.EmbeddingBag can enhance the performance and memory efficiency to process a sequence of tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWiF5WDj7VVK"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQYxfdZc7eWm"
      },
      "source": [
        "We build a model with the embedding dimension of 64. The vocab size is equal to the length of the vocabulary instance. The number of classes is equal to the number of labels,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WCp2e_C7Z5S"
      },
      "source": [
        "train_iter = YahooAnswers(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRLyvS-f7f-D"
      },
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtzEbdi17rDd"
      },
      "source": [
        "Split the dataset and run the model\n",
        "Since the original AG_NEWS has no valid dataset, we split the training dataset into train/valid sets with a split ratio of 0.95 (train) and 0.05 (valid). Here we use torch.utils.data.dataset.random_split <https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split>__ function in PyTorch core library.\n",
        "\n",
        "CrossEntropyLoss <https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>__ criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class. It is useful when training a classification problem with C classes. SGD <https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html>__ implements stochastic gradient descent method as the optimizer. The initial learning rate is set to 5.0. StepLR <https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR>__ is used here to adjust the learning rate through epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkoEyB2b7itr",
        "outputId": "0f82d539-8b09-422e-e1e0-2b4b268a4e5e"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = YahooAnswers()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/20782 batches | accuracy    0.285\n",
            "| epoch   1 |  1000/20782 batches | accuracy    0.437\n",
            "| epoch   1 |  1500/20782 batches | accuracy    0.514\n",
            "| epoch   1 |  2000/20782 batches | accuracy    0.551\n",
            "| epoch   1 |  2500/20782 batches | accuracy    0.575\n",
            "| epoch   1 |  3000/20782 batches | accuracy    0.593\n",
            "| epoch   1 |  3500/20782 batches | accuracy    0.608\n",
            "| epoch   1 |  4000/20782 batches | accuracy    0.615\n",
            "| epoch   1 |  4500/20782 batches | accuracy    0.624\n",
            "| epoch   1 |  5000/20782 batches | accuracy    0.629\n",
            "| epoch   1 |  5500/20782 batches | accuracy    0.634\n",
            "| epoch   1 |  6000/20782 batches | accuracy    0.642\n",
            "| epoch   1 |  6500/20782 batches | accuracy    0.644\n",
            "| epoch   1 |  7000/20782 batches | accuracy    0.650\n",
            "| epoch   1 |  7500/20782 batches | accuracy    0.652\n",
            "| epoch   1 |  8000/20782 batches | accuracy    0.654\n",
            "| epoch   1 |  8500/20782 batches | accuracy    0.654\n",
            "| epoch   1 |  9000/20782 batches | accuracy    0.650\n",
            "| epoch   1 |  9500/20782 batches | accuracy    0.657\n",
            "| epoch   1 | 10000/20782 batches | accuracy    0.659\n",
            "| epoch   1 | 10500/20782 batches | accuracy    0.661\n",
            "| epoch   1 | 11000/20782 batches | accuracy    0.664\n",
            "| epoch   1 | 11500/20782 batches | accuracy    0.666\n",
            "| epoch   1 | 12000/20782 batches | accuracy    0.668\n",
            "| epoch   1 | 12500/20782 batches | accuracy    0.664\n",
            "| epoch   1 | 13000/20782 batches | accuracy    0.669\n",
            "| epoch   1 | 13500/20782 batches | accuracy    0.671\n",
            "| epoch   1 | 14000/20782 batches | accuracy    0.675\n",
            "| epoch   1 | 14500/20782 batches | accuracy    0.670\n",
            "| epoch   1 | 15000/20782 batches | accuracy    0.669\n",
            "| epoch   1 | 15500/20782 batches | accuracy    0.670\n",
            "| epoch   1 | 16000/20782 batches | accuracy    0.676\n",
            "| epoch   1 | 16500/20782 batches | accuracy    0.674\n",
            "| epoch   1 | 17000/20782 batches | accuracy    0.678\n",
            "| epoch   1 | 17500/20782 batches | accuracy    0.673\n",
            "| epoch   1 | 18000/20782 batches | accuracy    0.680\n",
            "| epoch   1 | 18500/20782 batches | accuracy    0.678\n",
            "| epoch   1 | 19000/20782 batches | accuracy    0.680\n",
            "| epoch   1 | 19500/20782 batches | accuracy    0.681\n",
            "| epoch   1 | 20000/20782 batches | accuracy    0.682\n",
            "| epoch   1 | 20500/20782 batches | accuracy    0.682\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 326.14s | valid accuracy    0.681 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/20782 batches | accuracy    0.681\n",
            "| epoch   2 |  1000/20782 batches | accuracy    0.682\n",
            "| epoch   2 |  1500/20782 batches | accuracy    0.686\n",
            "| epoch   2 |  2000/20782 batches | accuracy    0.682\n",
            "| epoch   2 |  2500/20782 batches | accuracy    0.684\n",
            "| epoch   2 |  3000/20782 batches | accuracy    0.686\n",
            "| epoch   2 |  3500/20782 batches | accuracy    0.687\n",
            "| epoch   2 |  4000/20782 batches | accuracy    0.686\n",
            "| epoch   2 |  4500/20782 batches | accuracy    0.687\n",
            "| epoch   2 |  5000/20782 batches | accuracy    0.688\n",
            "| epoch   2 |  5500/20782 batches | accuracy    0.689\n",
            "| epoch   2 |  6000/20782 batches | accuracy    0.693\n",
            "| epoch   2 |  6500/20782 batches | accuracy    0.691\n",
            "| epoch   2 |  7000/20782 batches | accuracy    0.690\n",
            "| epoch   2 |  7500/20782 batches | accuracy    0.692\n",
            "| epoch   2 |  8000/20782 batches | accuracy    0.690\n",
            "| epoch   2 |  8500/20782 batches | accuracy    0.692\n",
            "| epoch   2 |  9000/20782 batches | accuracy    0.688\n",
            "| epoch   2 |  9500/20782 batches | accuracy    0.689\n",
            "| epoch   2 | 10000/20782 batches | accuracy    0.690\n",
            "| epoch   2 | 10500/20782 batches | accuracy    0.691\n",
            "| epoch   2 | 11000/20782 batches | accuracy    0.692\n",
            "| epoch   2 | 11500/20782 batches | accuracy    0.692\n",
            "| epoch   2 | 12000/20782 batches | accuracy    0.693\n",
            "| epoch   2 | 12500/20782 batches | accuracy    0.689\n",
            "| epoch   2 | 13000/20782 batches | accuracy    0.693\n",
            "| epoch   2 | 13500/20782 batches | accuracy    0.695\n",
            "| epoch   2 | 14000/20782 batches | accuracy    0.697\n",
            "| epoch   2 | 14500/20782 batches | accuracy    0.693\n",
            "| epoch   2 | 15000/20782 batches | accuracy    0.691\n",
            "| epoch   2 | 15500/20782 batches | accuracy    0.691\n",
            "| epoch   2 | 16000/20782 batches | accuracy    0.697\n",
            "| epoch   2 | 16500/20782 batches | accuracy    0.693\n",
            "| epoch   2 | 17000/20782 batches | accuracy    0.697\n",
            "| epoch   2 | 17500/20782 batches | accuracy    0.692\n",
            "| epoch   2 | 18000/20782 batches | accuracy    0.698\n",
            "| epoch   2 | 18500/20782 batches | accuracy    0.698\n",
            "| epoch   2 | 19000/20782 batches | accuracy    0.697\n",
            "| epoch   2 | 19500/20782 batches | accuracy    0.699\n",
            "| epoch   2 | 20000/20782 batches | accuracy    0.698\n",
            "| epoch   2 | 20500/20782 batches | accuracy    0.697\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 321.06s | valid accuracy    0.693 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/20782 batches | accuracy    0.696\n",
            "| epoch   3 |  1000/20782 batches | accuracy    0.696\n",
            "| epoch   3 |  1500/20782 batches | accuracy    0.701\n",
            "| epoch   3 |  2000/20782 batches | accuracy    0.696\n",
            "| epoch   3 |  2500/20782 batches | accuracy    0.697\n",
            "| epoch   3 |  3000/20782 batches | accuracy    0.699\n",
            "| epoch   3 |  3500/20782 batches | accuracy    0.700\n",
            "| epoch   3 |  4000/20782 batches | accuracy    0.700\n",
            "| epoch   3 |  4500/20782 batches | accuracy    0.700\n",
            "| epoch   3 |  5000/20782 batches | accuracy    0.700\n",
            "| epoch   3 |  5500/20782 batches | accuracy    0.701\n",
            "| epoch   3 |  6000/20782 batches | accuracy    0.707\n",
            "| epoch   3 |  6500/20782 batches | accuracy    0.703\n",
            "| epoch   3 |  7000/20782 batches | accuracy    0.704\n",
            "| epoch   3 |  7500/20782 batches | accuracy    0.704\n",
            "| epoch   3 |  8000/20782 batches | accuracy    0.701\n",
            "| epoch   3 |  8500/20782 batches | accuracy    0.702\n",
            "| epoch   3 |  9000/20782 batches | accuracy    0.699\n",
            "| epoch   3 |  9500/20782 batches | accuracy    0.700\n",
            "| epoch   3 | 10000/20782 batches | accuracy    0.701\n",
            "| epoch   3 | 10500/20782 batches | accuracy    0.702\n",
            "| epoch   3 | 11000/20782 batches | accuracy    0.703\n",
            "| epoch   3 | 11500/20782 batches | accuracy    0.703\n",
            "| epoch   3 | 12000/20782 batches | accuracy    0.703\n",
            "| epoch   3 | 12500/20782 batches | accuracy    0.700\n",
            "| epoch   3 | 13000/20782 batches | accuracy    0.703\n",
            "| epoch   3 | 13500/20782 batches | accuracy    0.706\n",
            "| epoch   3 | 14000/20782 batches | accuracy    0.706\n",
            "| epoch   3 | 14500/20782 batches | accuracy    0.703\n",
            "| epoch   3 | 15000/20782 batches | accuracy    0.701\n",
            "| epoch   3 | 15500/20782 batches | accuracy    0.700\n",
            "| epoch   3 | 16000/20782 batches | accuracy    0.705\n",
            "| epoch   3 | 16500/20782 batches | accuracy    0.702\n",
            "| epoch   3 | 17000/20782 batches | accuracy    0.706\n",
            "| epoch   3 | 17500/20782 batches | accuracy    0.702\n",
            "| epoch   3 | 18000/20782 batches | accuracy    0.707\n",
            "| epoch   3 | 18500/20782 batches | accuracy    0.706\n",
            "| epoch   3 | 19000/20782 batches | accuracy    0.706\n",
            "| epoch   3 | 19500/20782 batches | accuracy    0.708\n",
            "| epoch   3 | 20000/20782 batches | accuracy    0.707\n",
            "| epoch   3 | 20500/20782 batches | accuracy    0.706\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 325.29s | valid accuracy    0.698 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/20782 batches | accuracy    0.704\n",
            "| epoch   4 |  1000/20782 batches | accuracy    0.705\n",
            "| epoch   4 |  1500/20782 batches | accuracy    0.708\n",
            "| epoch   4 |  2000/20782 batches | accuracy    0.704\n",
            "| epoch   4 |  2500/20782 batches | accuracy    0.705\n",
            "| epoch   4 |  3000/20782 batches | accuracy    0.705\n",
            "| epoch   4 |  3500/20782 batches | accuracy    0.708\n",
            "| epoch   4 |  4000/20782 batches | accuracy    0.709\n",
            "| epoch   4 |  4500/20782 batches | accuracy    0.707\n",
            "| epoch   4 |  5000/20782 batches | accuracy    0.707\n",
            "| epoch   4 |  5500/20782 batches | accuracy    0.708\n",
            "| epoch   4 |  6000/20782 batches | accuracy    0.713\n",
            "| epoch   4 |  6500/20782 batches | accuracy    0.710\n",
            "| epoch   4 |  7000/20782 batches | accuracy    0.710\n",
            "| epoch   4 |  7500/20782 batches | accuracy    0.709\n",
            "| epoch   4 |  8000/20782 batches | accuracy    0.708\n",
            "| epoch   4 |  8500/20782 batches | accuracy    0.709\n",
            "| epoch   4 |  9000/20782 batches | accuracy    0.707\n",
            "| epoch   4 |  9500/20782 batches | accuracy    0.706\n",
            "| epoch   4 | 10000/20782 batches | accuracy    0.708\n",
            "| epoch   4 | 10500/20782 batches | accuracy    0.708\n",
            "| epoch   4 | 11000/20782 batches | accuracy    0.711\n",
            "| epoch   4 | 11500/20782 batches | accuracy    0.709\n",
            "| epoch   4 | 12000/20782 batches | accuracy    0.708\n",
            "| epoch   4 | 12500/20782 batches | accuracy    0.707\n",
            "| epoch   4 | 13000/20782 batches | accuracy    0.709\n",
            "| epoch   4 | 13500/20782 batches | accuracy    0.712\n",
            "| epoch   4 | 14000/20782 batches | accuracy    0.713\n",
            "| epoch   4 | 14500/20782 batches | accuracy    0.708\n",
            "| epoch   4 | 15000/20782 batches | accuracy    0.707\n",
            "| epoch   4 | 15500/20782 batches | accuracy    0.706\n",
            "| epoch   4 | 16000/20782 batches | accuracy    0.711\n",
            "| epoch   4 | 16500/20782 batches | accuracy    0.708\n",
            "| epoch   4 | 17000/20782 batches | accuracy    0.712\n",
            "| epoch   4 | 17500/20782 batches | accuracy    0.708\n",
            "| epoch   4 | 18000/20782 batches | accuracy    0.713\n",
            "| epoch   4 | 18500/20782 batches | accuracy    0.713\n",
            "| epoch   4 | 19000/20782 batches | accuracy    0.712\n",
            "| epoch   4 | 19500/20782 batches | accuracy    0.714\n",
            "| epoch   4 | 20000/20782 batches | accuracy    0.711\n",
            "| epoch   4 | 20500/20782 batches | accuracy    0.712\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 345.74s | valid accuracy    0.701 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/20782 batches | accuracy    0.709\n",
            "| epoch   5 |  1000/20782 batches | accuracy    0.710\n",
            "| epoch   5 |  1500/20782 batches | accuracy    0.714\n",
            "| epoch   5 |  2000/20782 batches | accuracy    0.710\n",
            "| epoch   5 |  2500/20782 batches | accuracy    0.710\n",
            "| epoch   5 |  3000/20782 batches | accuracy    0.711\n",
            "| epoch   5 |  3500/20782 batches | accuracy    0.712\n",
            "| epoch   5 |  4000/20782 batches | accuracy    0.713\n",
            "| epoch   5 |  4500/20782 batches | accuracy    0.712\n",
            "| epoch   5 |  5000/20782 batches | accuracy    0.712\n",
            "| epoch   5 |  5500/20782 batches | accuracy    0.714\n",
            "| epoch   5 |  6000/20782 batches | accuracy    0.719\n",
            "| epoch   5 |  6500/20782 batches | accuracy    0.715\n",
            "| epoch   5 |  7000/20782 batches | accuracy    0.714\n",
            "| epoch   5 |  7500/20782 batches | accuracy    0.715\n",
            "| epoch   5 |  8000/20782 batches | accuracy    0.714\n",
            "| epoch   5 |  8500/20782 batches | accuracy    0.713\n",
            "| epoch   5 |  9000/20782 batches | accuracy    0.712\n",
            "| epoch   5 |  9500/20782 batches | accuracy    0.711\n",
            "| epoch   5 | 10000/20782 batches | accuracy    0.711\n",
            "| epoch   5 | 10500/20782 batches | accuracy    0.713\n",
            "| epoch   5 | 11000/20782 batches | accuracy    0.716\n",
            "| epoch   5 | 11500/20782 batches | accuracy    0.715\n",
            "| epoch   5 | 12000/20782 batches | accuracy    0.712\n",
            "| epoch   5 | 12500/20782 batches | accuracy    0.711\n",
            "| epoch   5 | 13000/20782 batches | accuracy    0.714\n",
            "| epoch   5 | 13500/20782 batches | accuracy    0.717\n",
            "| epoch   5 | 14000/20782 batches | accuracy    0.718\n",
            "| epoch   5 | 14500/20782 batches | accuracy    0.713\n",
            "| epoch   5 | 15000/20782 batches | accuracy    0.711\n",
            "| epoch   5 | 15500/20782 batches | accuracy    0.710\n",
            "| epoch   5 | 16000/20782 batches | accuracy    0.716\n",
            "| epoch   5 | 16500/20782 batches | accuracy    0.712\n",
            "| epoch   5 | 17000/20782 batches | accuracy    0.716\n",
            "| epoch   5 | 17500/20782 batches | accuracy    0.714\n",
            "| epoch   5 | 18000/20782 batches | accuracy    0.716\n",
            "| epoch   5 | 18500/20782 batches | accuracy    0.716\n",
            "| epoch   5 | 19000/20782 batches | accuracy    0.717\n",
            "| epoch   5 | 19500/20782 batches | accuracy    0.718\n",
            "| epoch   5 | 20000/20782 batches | accuracy    0.716\n",
            "| epoch   5 | 20500/20782 batches | accuracy    0.716\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 323.70s | valid accuracy    0.702 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/20782 batches | accuracy    0.713\n",
            "| epoch   6 |  1000/20782 batches | accuracy    0.715\n",
            "| epoch   6 |  1500/20782 batches | accuracy    0.717\n",
            "| epoch   6 |  2000/20782 batches | accuracy    0.714\n",
            "| epoch   6 |  2500/20782 batches | accuracy    0.714\n",
            "| epoch   6 |  3000/20782 batches | accuracy    0.715\n",
            "| epoch   6 |  3500/20782 batches | accuracy    0.718\n",
            "| epoch   6 |  4000/20782 batches | accuracy    0.717\n",
            "| epoch   6 |  4500/20782 batches | accuracy    0.717\n",
            "| epoch   6 |  5000/20782 batches | accuracy    0.717\n",
            "| epoch   6 |  5500/20782 batches | accuracy    0.718\n",
            "| epoch   6 |  6000/20782 batches | accuracy    0.723\n",
            "| epoch   6 |  6500/20782 batches | accuracy    0.720\n",
            "| epoch   6 |  7000/20782 batches | accuracy    0.719\n",
            "| epoch   6 |  7500/20782 batches | accuracy    0.719\n",
            "| epoch   6 |  8000/20782 batches | accuracy    0.719\n",
            "| epoch   6 |  8500/20782 batches | accuracy    0.717\n",
            "| epoch   6 |  9000/20782 batches | accuracy    0.716\n",
            "| epoch   6 |  9500/20782 batches | accuracy    0.716\n",
            "| epoch   6 | 10000/20782 batches | accuracy    0.715\n",
            "| epoch   6 | 10500/20782 batches | accuracy    0.717\n",
            "| epoch   6 | 11000/20782 batches | accuracy    0.720\n",
            "| epoch   6 | 11500/20782 batches | accuracy    0.720\n",
            "| epoch   6 | 12000/20782 batches | accuracy    0.716\n",
            "| epoch   6 | 12500/20782 batches | accuracy    0.714\n",
            "| epoch   6 | 13000/20782 batches | accuracy    0.718\n",
            "| epoch   6 | 13500/20782 batches | accuracy    0.721\n",
            "| epoch   6 | 14000/20782 batches | accuracy    0.722\n",
            "| epoch   6 | 14500/20782 batches | accuracy    0.717\n",
            "| epoch   6 | 15000/20782 batches | accuracy    0.715\n",
            "| epoch   6 | 15500/20782 batches | accuracy    0.713\n",
            "| epoch   6 | 16000/20782 batches | accuracy    0.720\n",
            "| epoch   6 | 16500/20782 batches | accuracy    0.717\n",
            "| epoch   6 | 17000/20782 batches | accuracy    0.720\n",
            "| epoch   6 | 17500/20782 batches | accuracy    0.716\n",
            "| epoch   6 | 18000/20782 batches | accuracy    0.720\n",
            "| epoch   6 | 18500/20782 batches | accuracy    0.721\n",
            "| epoch   6 | 19000/20782 batches | accuracy    0.720\n",
            "| epoch   6 | 19500/20782 batches | accuracy    0.722\n",
            "| epoch   6 | 20000/20782 batches | accuracy    0.719\n",
            "| epoch   6 | 20500/20782 batches | accuracy    0.720\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 321.83s | valid accuracy    0.704 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/20782 batches | accuracy    0.718\n",
            "| epoch   7 |  1000/20782 batches | accuracy    0.719\n",
            "| epoch   7 |  1500/20782 batches | accuracy    0.721\n",
            "| epoch   7 |  2000/20782 batches | accuracy    0.718\n",
            "| epoch   7 |  2500/20782 batches | accuracy    0.718\n",
            "| epoch   7 |  3000/20782 batches | accuracy    0.719\n",
            "| epoch   7 |  3500/20782 batches | accuracy    0.721\n",
            "| epoch   7 |  4000/20782 batches | accuracy    0.720\n",
            "| epoch   7 |  4500/20782 batches | accuracy    0.721\n",
            "| epoch   7 |  5000/20782 batches | accuracy    0.720\n",
            "| epoch   7 |  5500/20782 batches | accuracy    0.722\n",
            "| epoch   7 |  6000/20782 batches | accuracy    0.726\n",
            "| epoch   7 |  6500/20782 batches | accuracy    0.723\n",
            "| epoch   7 |  7000/20782 batches | accuracy    0.721\n",
            "| epoch   7 |  7500/20782 batches | accuracy    0.722\n",
            "| epoch   7 |  8000/20782 batches | accuracy    0.722\n",
            "| epoch   7 |  8500/20782 batches | accuracy    0.720\n",
            "| epoch   7 |  9000/20782 batches | accuracy    0.719\n",
            "| epoch   7 |  9500/20782 batches | accuracy    0.720\n",
            "| epoch   7 | 10000/20782 batches | accuracy    0.719\n",
            "| epoch   7 | 10500/20782 batches | accuracy    0.720\n",
            "| epoch   7 | 11000/20782 batches | accuracy    0.723\n",
            "| epoch   7 | 11500/20782 batches | accuracy    0.723\n",
            "| epoch   7 | 12000/20782 batches | accuracy    0.720\n",
            "| epoch   7 | 12500/20782 batches | accuracy    0.717\n",
            "| epoch   7 | 13000/20782 batches | accuracy    0.722\n",
            "| epoch   7 | 13500/20782 batches | accuracy    0.724\n",
            "| epoch   7 | 14000/20782 batches | accuracy    0.726\n",
            "| epoch   7 | 14500/20782 batches | accuracy    0.721\n",
            "| epoch   7 | 15000/20782 batches | accuracy    0.718\n",
            "| epoch   7 | 15500/20782 batches | accuracy    0.716\n",
            "| epoch   7 | 16000/20782 batches | accuracy    0.723\n",
            "| epoch   7 | 16500/20782 batches | accuracy    0.720\n",
            "| epoch   7 | 17000/20782 batches | accuracy    0.723\n",
            "| epoch   7 | 17500/20782 batches | accuracy    0.719\n",
            "| epoch   7 | 18000/20782 batches | accuracy    0.722\n",
            "| epoch   7 | 18500/20782 batches | accuracy    0.725\n",
            "| epoch   7 | 19000/20782 batches | accuracy    0.724\n",
            "| epoch   7 | 19500/20782 batches | accuracy    0.725\n",
            "| epoch   7 | 20000/20782 batches | accuracy    0.722\n",
            "| epoch   7 | 20500/20782 batches | accuracy    0.724\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 323.91s | valid accuracy    0.705 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/20782 batches | accuracy    0.721\n",
            "| epoch   8 |  1000/20782 batches | accuracy    0.722\n",
            "| epoch   8 |  1500/20782 batches | accuracy    0.725\n",
            "| epoch   8 |  2000/20782 batches | accuracy    0.721\n",
            "| epoch   8 |  2500/20782 batches | accuracy    0.721\n",
            "| epoch   8 |  3000/20782 batches | accuracy    0.722\n",
            "| epoch   8 |  3500/20782 batches | accuracy    0.724\n",
            "| epoch   8 |  4000/20782 batches | accuracy    0.723\n",
            "| epoch   8 |  4500/20782 batches | accuracy    0.724\n",
            "| epoch   8 |  5000/20782 batches | accuracy    0.724\n",
            "| epoch   8 |  5500/20782 batches | accuracy    0.725\n",
            "| epoch   8 |  6000/20782 batches | accuracy    0.728\n",
            "| epoch   8 |  6500/20782 batches | accuracy    0.726\n",
            "| epoch   8 |  7000/20782 batches | accuracy    0.725\n",
            "| epoch   8 |  7500/20782 batches | accuracy    0.725\n",
            "| epoch   8 |  8000/20782 batches | accuracy    0.725\n",
            "| epoch   8 |  8500/20782 batches | accuracy    0.723\n",
            "| epoch   8 |  9000/20782 batches | accuracy    0.722\n",
            "| epoch   8 |  9500/20782 batches | accuracy    0.723\n",
            "| epoch   8 | 10000/20782 batches | accuracy    0.722\n",
            "| epoch   8 | 10500/20782 batches | accuracy    0.723\n",
            "| epoch   8 | 11000/20782 batches | accuracy    0.725\n",
            "| epoch   8 | 11500/20782 batches | accuracy    0.726\n",
            "| epoch   8 | 12000/20782 batches | accuracy    0.723\n",
            "| epoch   8 | 12500/20782 batches | accuracy    0.720\n",
            "| epoch   8 | 13000/20782 batches | accuracy    0.724\n",
            "| epoch   8 | 13500/20782 batches | accuracy    0.727\n",
            "| epoch   8 | 14000/20782 batches | accuracy    0.728\n",
            "| epoch   8 | 14500/20782 batches | accuracy    0.724\n",
            "| epoch   8 | 15000/20782 batches | accuracy    0.721\n",
            "| epoch   8 | 15500/20782 batches | accuracy    0.720\n",
            "| epoch   8 | 16000/20782 batches | accuracy    0.725\n",
            "| epoch   8 | 16500/20782 batches | accuracy    0.723\n",
            "| epoch   8 | 17000/20782 batches | accuracy    0.726\n",
            "| epoch   8 | 17500/20782 batches | accuracy    0.722\n",
            "| epoch   8 | 18000/20782 batches | accuracy    0.725\n",
            "| epoch   8 | 18500/20782 batches | accuracy    0.727\n",
            "| epoch   8 | 19000/20782 batches | accuracy    0.727\n",
            "| epoch   8 | 19500/20782 batches | accuracy    0.729\n",
            "| epoch   8 | 20000/20782 batches | accuracy    0.725\n",
            "| epoch   8 | 20500/20782 batches | accuracy    0.726\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 315.67s | valid accuracy    0.705 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/20782 batches | accuracy    0.724\n",
            "| epoch   9 |  1000/20782 batches | accuracy    0.725\n",
            "| epoch   9 |  1500/20782 batches | accuracy    0.727\n",
            "| epoch   9 |  2000/20782 batches | accuracy    0.724\n",
            "| epoch   9 |  2500/20782 batches | accuracy    0.724\n",
            "| epoch   9 |  3000/20782 batches | accuracy    0.724\n",
            "| epoch   9 |  3500/20782 batches | accuracy    0.726\n",
            "| epoch   9 |  4000/20782 batches | accuracy    0.727\n",
            "| epoch   9 |  4500/20782 batches | accuracy    0.727\n",
            "| epoch   9 |  5000/20782 batches | accuracy    0.726\n",
            "| epoch   9 |  5500/20782 batches | accuracy    0.728\n",
            "| epoch   9 |  6000/20782 batches | accuracy    0.731\n",
            "| epoch   9 |  6500/20782 batches | accuracy    0.728\n",
            "| epoch   9 |  7000/20782 batches | accuracy    0.728\n",
            "| epoch   9 |  7500/20782 batches | accuracy    0.729\n",
            "| epoch   9 |  8000/20782 batches | accuracy    0.728\n",
            "| epoch   9 |  8500/20782 batches | accuracy    0.725\n",
            "| epoch   9 |  9000/20782 batches | accuracy    0.725\n",
            "| epoch   9 |  9500/20782 batches | accuracy    0.725\n",
            "| epoch   9 | 10000/20782 batches | accuracy    0.724\n",
            "| epoch   9 | 10500/20782 batches | accuracy    0.725\n",
            "| epoch   9 | 11000/20782 batches | accuracy    0.728\n",
            "| epoch   9 | 11500/20782 batches | accuracy    0.728\n",
            "| epoch   9 | 12000/20782 batches | accuracy    0.725\n",
            "| epoch   9 | 12500/20782 batches | accuracy    0.723\n",
            "| epoch   9 | 13000/20782 batches | accuracy    0.727\n",
            "| epoch   9 | 13500/20782 batches | accuracy    0.730\n",
            "| epoch   9 | 14000/20782 batches | accuracy    0.731\n",
            "| epoch   9 | 14500/20782 batches | accuracy    0.727\n",
            "| epoch   9 | 15000/20782 batches | accuracy    0.724\n",
            "| epoch   9 | 15500/20782 batches | accuracy    0.723\n",
            "| epoch   9 | 16000/20782 batches | accuracy    0.729\n",
            "| epoch   9 | 16500/20782 batches | accuracy    0.726\n",
            "| epoch   9 | 17000/20782 batches | accuracy    0.728\n",
            "| epoch   9 | 17500/20782 batches | accuracy    0.725\n",
            "| epoch   9 | 18000/20782 batches | accuracy    0.727\n",
            "| epoch   9 | 18500/20782 batches | accuracy    0.730\n",
            "| epoch   9 | 19000/20782 batches | accuracy    0.729\n",
            "| epoch   9 | 19500/20782 batches | accuracy    0.732\n",
            "| epoch   9 | 20000/20782 batches | accuracy    0.727\n",
            "| epoch   9 | 20500/20782 batches | accuracy    0.728\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 315.86s | valid accuracy    0.706 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/20782 batches | accuracy    0.727\n",
            "| epoch  10 |  1000/20782 batches | accuracy    0.727\n",
            "| epoch  10 |  1500/20782 batches | accuracy    0.730\n",
            "| epoch  10 |  2000/20782 batches | accuracy    0.726\n",
            "| epoch  10 |  2500/20782 batches | accuracy    0.726\n",
            "| epoch  10 |  3000/20782 batches | accuracy    0.727\n",
            "| epoch  10 |  3500/20782 batches | accuracy    0.729\n",
            "| epoch  10 |  4000/20782 batches | accuracy    0.729\n",
            "| epoch  10 |  4500/20782 batches | accuracy    0.730\n",
            "| epoch  10 |  5000/20782 batches | accuracy    0.729\n",
            "| epoch  10 |  5500/20782 batches | accuracy    0.730\n",
            "| epoch  10 |  6000/20782 batches | accuracy    0.734\n",
            "| epoch  10 |  6500/20782 batches | accuracy    0.731\n",
            "| epoch  10 |  7000/20782 batches | accuracy    0.731\n",
            "| epoch  10 |  7500/20782 batches | accuracy    0.731\n",
            "| epoch  10 |  8000/20782 batches | accuracy    0.731\n",
            "| epoch  10 |  8500/20782 batches | accuracy    0.728\n",
            "| epoch  10 |  9000/20782 batches | accuracy    0.728\n",
            "| epoch  10 |  9500/20782 batches | accuracy    0.728\n",
            "| epoch  10 | 10000/20782 batches | accuracy    0.727\n",
            "| epoch  10 | 10500/20782 batches | accuracy    0.728\n",
            "| epoch  10 | 11000/20782 batches | accuracy    0.731\n",
            "| epoch  10 | 11500/20782 batches | accuracy    0.731\n",
            "| epoch  10 | 12000/20782 batches | accuracy    0.728\n",
            "| epoch  10 | 12500/20782 batches | accuracy    0.725\n",
            "| epoch  10 | 13000/20782 batches | accuracy    0.730\n",
            "| epoch  10 | 13500/20782 batches | accuracy    0.732\n",
            "| epoch  10 | 14000/20782 batches | accuracy    0.733\n",
            "| epoch  10 | 14500/20782 batches | accuracy    0.729\n",
            "| epoch  10 | 15000/20782 batches | accuracy    0.726\n",
            "| epoch  10 | 15500/20782 batches | accuracy    0.725\n",
            "| epoch  10 | 16000/20782 batches | accuracy    0.731\n",
            "| epoch  10 | 16500/20782 batches | accuracy    0.728\n",
            "| epoch  10 | 17000/20782 batches | accuracy    0.730\n",
            "| epoch  10 | 17500/20782 batches | accuracy    0.728\n",
            "| epoch  10 | 18000/20782 batches | accuracy    0.730\n",
            "| epoch  10 | 18500/20782 batches | accuracy    0.732\n",
            "| epoch  10 | 19000/20782 batches | accuracy    0.732\n",
            "| epoch  10 | 19500/20782 batches | accuracy    0.734\n",
            "| epoch  10 | 20000/20782 batches | accuracy    0.730\n",
            "| epoch  10 | 20500/20782 batches | accuracy    0.731\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 308.68s | valid accuracy    0.707 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbksWIvf7mQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94270761-7e03-4a77-f3aa-48ce353be27a"
      },
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbyPGkgBRpmF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}