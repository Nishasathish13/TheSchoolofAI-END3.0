{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 5-Assignment_dataset1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishasathish13/TheSchoolofAI-END3.0/blob/main/Session%205%20-%20TorchText%20%26%20Advanced%20Concepts/Assignment/Session_5_Assignment_dataset1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VaVIQPawdbU"
      },
      "source": [
        "from torchtext.datasets import YelpReviewPolarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIDrrV3AcpNu",
        "outputId": "3848d946-6da6-49be-aaac-58787759c6af"
      },
      "source": [
        "help(YelpReviewPolarity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function YelpReviewPolarity in module torchtext.datasets.yelpreviewpolarity:\n",
            "\n",
            "YelpReviewPolarity(root='.data', split=('train', 'test'))\n",
            "    YelpReviewPolarity dataset\n",
            "    \n",
            "    Separately returns the train/test split\n",
            "    \n",
            "    Number of lines per split:\n",
            "        train: 560000\n",
            "    \n",
            "        test: 38000\n",
            "    \n",
            "    \n",
            "    Number of classes\n",
            "        2\n",
            "    \n",
            "    \n",
            "    Args:\n",
            "        root: Directory where the datasets are saved.\n",
            "            Default: .data\n",
            "        split: split or splits to be returned. Can be a string or tuple of strings.\n",
            "            Default: ('train', 'test')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBvZyNEAwtgf",
        "outputId": "abe03765-5a6f-4223-d442-92cc9fc943a3"
      },
      "source": [
        "train_iter = YelpReviewPolarity(split='train')\n",
        "next(train_iter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 166M/166M [00:04<00:00, 39.6MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " \"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrC7Ywp0w2j9",
        "outputId": "361d0a53-14f4-4e68-c2e8-635b767b281d"
      },
      "source": [
        "# or iterate over a loop\n",
        "\n",
        "#to display only the first 20 lines\n",
        "for (ln, (label, line)) in enumerate(train_iter):\n",
        "  print(ln, label, line)\n",
        "  if ln == 20:\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2 Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn't judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life.\n",
            "1 1 I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. I was going to Dr. Johnson before he left and Goldberg took over when Johnson left. He is not a caring doctor. He is only interested in the co-pay and having you come in for medication refills every month. He will not give refills and could less about patients's financial situations. Trying to get your 90 days mail away pharmacy prescriptions through this guy is a joke. And to make matters even worse, his office staff is incompetent. 90% of the time when you call the office, they'll put you through to a voice mail, that NO ONE ever answers or returns your call. Both my adult children and husband have decided to leave this practice after experiencing such frustration. The entire office has an attitude like they are doing you a favor. Give me a break! Stay away from this doc and the practice. You deserve better and they will not be there when you really need them. I have never felt compelled to write a bad review about anyone until I met this pathetic excuse for a doctor who is all about the money.\n",
            "2 1 I'm writing this review to give you a heads up before you see this Doctor. The office staff and administration are very unprofessional. I left a message with multiple people regarding my bill, and no one ever called me back. I had to hound them to get an answer about my bill. \\n\\nSecond, and most important, make sure your insurance is going to cover Dr. Goldberg's visits and blood work. He recommended to me that I get a physical, and he knew I was a student because I told him. I got the physical done. Later, I found out my health insurance doesn't pay for preventative visits. I received an $800.00 bill for the blood work. I can't pay for my bill because I'm a student and don't have any cash flow at this current time. I can't believe the Doctor wouldn't give me a heads up to make sure my insurance would cover work that wasn't necessary and was strictly preventative. The office can't do anything to help me cover the bill. In addition, the office staff said the onus is on me to make sure my insurance covers visits. Frustrating situation!\n",
            "3 2 All the food is great here. But the best thing they have is their wings. Their wings are simply fantastic!!  The \\\"Wet Cajun\\\" are by the best & most popular.  I also like the seasoned salt wings.  Wing Night is Monday & Wednesday night, $0.75 whole wings!\\n\\nThe dining area is nice. Very family friendly! The bar is very nice is well.  This place is truly a Yinzer's dream!!  \\\"Pittsburgh Dad\\\" would love this place n'at!!\n",
            "4 1 Wing sauce is like water. Pretty much a lot of butter and some hot sauce (franks red hot maybe).  The whole wings are good size and crispy, but for $1 a wing the sauce could be better. The hot and extra hot are about the same flavor/heat.  The fish sandwich is good and is a large portion, sides are decent.\n",
            "5 1 Owning a driving range inside the city limits is like a license to print money.  I don't think I ask much out of a driving range.  Decent mats, clean balls and accessible hours.  Hell you need even less people now with the advent of the machine that doles out the balls.  This place has none of them.  It is april and there are no grass tees yet.  BTW they opened for the season this week although it has been golfing weather for a month.  The mats look like the carpet at my 107 year old aunt Irene's house.  Worn and thread bare.  Let's talk about the hours.  This place is equipped with lights yet they only sell buckets of balls until 730.  It is still light out.  Finally lets you have the pit to hit into.  When I arrived I wasn't sure if this was a driving range or an excavation site for a mastodon or a strip mining operation.  There is no grass on the range. Just mud.  Makes it a good tool to figure out how far you actually are hitting the ball.  Oh, they are cash only also.\\n\\nBottom line, this place sucks.  The best hope is that the owner sells it to someone that actually wants to make money and service golfers in Pittsburgh.\n",
            "6 1 This place is absolute garbage...  Half of the tees are not available, including all the grass tees.  It is cash only, and they sell the last bucket at 8, despite having lights.  And if you finish even a minute after 8, don't plan on getting a drink.  The vending machines are sold out (of course) and they sell drinks inside, but close the drawers at 8 on the dot.  There are weeds grown all over the place.  I noticed some sort of batting cage, but it looks like those are out of order as well.  Someone should buy this place and turn it into what it should be.\n",
            "7 2 Before I finally made it over to this range I heard the same thing from most people - it's just fine to go work on your swing. I had such a low expectation I was pleasantly surprised. \\n\\nIt's a fairly big range - if you are familiar with Scally's in Moon, it seems like it has almost as many tees, though its not nearly as nice a facility. \\n\\nThe guys in the pro shop were two of the friendlier guys I've come across at ranges or at courses. Yards were indeed marked and there are some targets to aim for, and even some hazards to aim away from. \\n\\nA big red flag to me was the extra charge ($3) to hit off the grass. I am no range expert, but this is the 4th one I've been to and the first I've seen of that sort of nickel and diming....\\n\\nPrice for the golf balls was reasonable and I do plan to be back every week until they close up in October for the season. Hopefully, since its for sale, it will reopen as a golf facility again.\n",
            "8 2 I drove by yesterday to get a sneak peak.  It re-opens on July 14th and I can't wait to take my kids.  The new range looks amazing.  The entire range appears to be turf, which may or many not help your game, but it looks really nice.  The tee boxes look state of the art and the club house looks like something you'll see on a newer course.  Can't wait to experience it!\n",
            "9 1 After waiting for almost 30 minutes to trade in an old phone part of the buy back program, our customer service rep incorrectly processed the transaction. This led to us waiting another 30 minutes for him to correct it. Don't visit this store if you want pleasant or good service.\n",
            "10 2 Wonderful reuben.  Map shown on Yelp page is incorrect. It is actually a different Hawkins.  I'd recommend a call for directions 412-271-9911.\n",
            "11 2 After a morning of Thrift Store hunting, a friend and I were thinking of lunch, and he suggested Emil's after he'd seen Chris Sebak do a bit on it and had tried it a time or two before, and I had not. He said they had a decent Reuben, but to be prepared to step back in time.\\n\\nWell, seeing as how I'm kind of addicted to late 40's and early 50's, and the whole Rat Pack scene, stepping back in time is a welcomed change in da burgh...as long as it doesn't involve 1979, which I can see all around me every day.\\n\\nAnd yet another shot at finding a decent Reuben in da burgh...well, that's like hunting the Holy Grail. So looking under one more bush certainly wouldn't hurt.\\n\\nSo off we go right at lunchtime in the middle of...where exactly were we? At first I thought we were lost, driving around a handful of very rather dismal looking blocks in what looked like a neighborhood that had been blighted by the building of a highway. And then...AHA! Here it is! And yep, there it was. This little unassuming building with an add-on entrance with what looked like a very old hand painted sign stating quite simply 'Emil's. \\n\\nWe walked in the front door, and entered another world. Another time, and another place. Oh, and any Big Burrito/Sousa foodies might as well stop reading now. I wouldn't want to see you walk in, roll your eyes and say 'Reaaaaaalllly?'\\n\\nThis is about as old world bar/lounge/restaurant as it gets. Plain, with a dark wood bar on one side, plain white walls with no yinzer pics, good sturdy chairs and actual white linens on the tables. This is the kind of neighborhood dive that I could see Frank and Dino pulling a few tables together for some poker, a fish sammich, and some cheap scotch. And THAT is exactly what I love.\\n\\nOh...but good food counts too. \\n\\nWe each had a Reuben, and my friend had a side of fries. The Reubens were decent, but not NY awesome. A little too thick on the bread, but overall, tasty and definitely filling. Not too skimpy on the meat. I seriously CRAVE a true, good NY Reuben, but since I can't afford to travel right now, what I find in da burgh will have to do. But as we sat and ate, burgers came out to an adjoining table. Those were some big thick burgers. A steak went past for the table behind us. That was HUGE! And when we asked about it, the waitress said 'Yeah, it's huge and really good, and he only charges $12.99 for it, ain't that nuts?' Another table of five came in, and wham. Fish sandwiches PILED with breaded fish that looked amazing. Yeah, I want that, that, that and THAT!\\n\\nMy friend also mentioned that they have a Chicken Parm special one day of the week that is only served UNTIL 4 pm, and that it is fantastic. If only I could GET there on that week day before 4...\\n\\nThe waitress did a good job, especially since there was quite a growing crowd at lunchtime on a Saturday, and only one of her. She kept up and was very friendly. \\n\\nThey only have Pepsi products, so I had a brewed iced tea, which was very fresh, and she did pop by to ask about refills as often as she could. As the lunch hour went on, they were getting busy.\\n\\nEmil's is no frills, good portions, very reasonable prices, VERY comfortable neighborhood hole in the wall...kind of like Cheers, but in a blue collar neighborhood in the 1950's. Fan-freakin-tastic! I could feel at home here.\\n\\nYou definitely want to hit Mapquest or plug in your GPS though. I am not sure that I could find it again on my own...it really is a hidden gem. I will be making my friend take me back until I can memorize where the heck it is.\\n\\nAddendum: 2nd visit for the fish sandwich. Excellent. Truly. A pound of fish on a fish-shaped bun (as opposed to da burgh's seemingly popular hamburger bun). The fish was flavorful, the batter excellent, and for just $8. This may have been the best fish sandwich I've yet to have in da burgh.\n",
            "12 2 This is a hidden gem, no really. It took us forever to find but well worth it. It is right across the street from the Rankin Police Station. The menu has a wide selection, I really couldn't decide what I wanted but I went with the ribeye sandwich. I'm glad i did too. Huge sandwich! I added mushrooms, it was very flavorful. My boyfriend got the fish sandwich, he enjoyed it as well. Fast and friendly service. Will definitely be back.\n",
            "13 2 Awesome drink specials during happy hour. Fantastic wings that are crispy and delicious, wing night on Tuesday and Thursday!\n",
            "14 1 Very disappointed in the customer service. We ordered Reuben's  and wanted coleslaw instead of kraut. They charged us $3.00 for the coleslaw. We will not be back . The iced tea is also terrible tasting.\n",
            "15 1 Used to go there for tires, brakes, etc.  Their prices have gone way up-$400 for 4 mid-level tires for a Toyota.  Plus, 1 of the new tires went flat within 3 weeks.  Since they don\\\"t make appointments,  the wait to get the tire looked at was ~2 hours.  Sorry--can't wait that long to get a warranted repair,  They lost my business for good.\n",
            "16 1 I got 'new' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\nI took the tire over to Flynn's and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he'd give me a new tire \\\"this time\\\". \\nI will never go back to Flynn's b/c of the way this guy treated me and the simple fact that they gave me a used tire!\n",
            "17 1 Terrible. Preordered my tires and when I arrived they couldn't find the order anywhere. Once we got through that process I waited over 2 hours for them to be put on... I was originally told it would take 30 mins. Slow, over priced, I'll go elsewhere next time.\n",
            "18 2 I've been informed by a fellow Yelper that they are just closed for the season, I hope they're right! I find it strange they don't keep their phone number in service with a recording and that they didn't cover the course up to protect it from weather, vandalism, etc. Either way, I can't wait to get my game on in the spring!\n",
            "19 1 Don't waste your time.  We had two different people come to our house to give us estimates for a deck (one of them the OWNER).  Both times, we never heard from them.  Not a call, not the estimate, nothing.\n",
            "20 1 I will start by saying we have a nice new deck. That is where the good part ends.\\n\\nWhy two stars if we have a nice deck now? Well, it all started in May when we had a man named Al come out and talk to us about the process. He was a true salesman, I'll tell you that. Not a straight shooter, but an excited salesman that wanted to sell... And we bought into it.\\n\\nHe told us they were backed up with the harsh winter and a lot of jobs, but they could probably start by June 24th. Mind you, this was the end of May we met with him. He said they would have to do a survey of our land and get a permit from Dormont before they started. We felt confident in going with them, and booked it.\\n\\nHere's where things fell apart. Weeks went by with no word from them on the process. Finally I called and left messages. Al finally got in touch with me and the conversation was utterly confusing. He must have not understood me because he just kept talking in circles. So I hung up not knowing what was going on. Finally I called back and the admin told me that the survey could take a couple weeks to come back. I wish Al would have told us that.\\n\\nSo our June 24th date came and went. We finally got our survey. And they were quick to bill us for it, but no word on where we were in the process. I called back a couple times with no real answers from the admin. Apparently they were now waiting for the permit from Dormont.\\n\\nA month later, I was frustrated and asked to speak with someone at the company. I spoke with someone who told me that Dormont usually takes a week to process the permit, but we were now going on a month. He told me he would bug the guy about it and we finally got it. \\n\\nI completely understand that waiting on the permit is not their fault, but why no communication about it? Just call me and tell me they're waiting for the permit, but they'll work towards asking Dormont what the hold up is. Seems simple.\\n\\nFinally we get the permit and work is supposed to start. Tuesday comes and they have to wait for approval from Dormont on holes they're digging. So holes are dug and equipment is put in my backyard and then nothing happens for two days. Where did everyone go? Who knows, because I wasn't informed! \\n\\nI was upset and contacted the company owner, Dave, by email. I told him how frustrated I was about the lack of communication. He wrote me two sentences that  said \\\"there will be acres there tomorrow morning to finish and most will be done tomorrow.\\\" Well, that didn't happen.\\n\\nThen two other dudes show up two days later and did work for about three hours and left. The main guy comes back the next day and says they did it wrong. So apparently the part about how they wouldn't need access to my house changes to they're drilling bolts into my house to secure he deck. \\n\\nFinally, the second week into construction, my tiny 10x11 foot deck is done. It was upsetting that they left the deck dirty, and all the dirt and cement they moved around wasn't cleaned up. I even asked them to fill all the holes they dug up and didn't use, and they only filled up one. When you create a construction project, isn't it just good business to clean it up and make it look nice? Didn't happen here.\\n\\nMy main point is, contractors have a reputation for not being the most trustworthy. Why be that company? Why not be an upstanding company that consumers can trust? My main issues with them were that things were not communicated to me properly up front, nor was the ongoing process communicated to me. Then they left everything unfinished in my eyes, leaving the site dirty. I understand things happen and things get delayed, but just email or call me. It's not that hard to keep me updated, it is 2014 with a million ways to be contacted.\\n\\nEven though I have a nice deck (after I clean things up myself!), I would not recommend them to anyone.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDP3CqFKxZ7X",
        "outputId": "bd232dde-9c6b-4710-dec0-dbf2697f7249"
      },
      "source": [
        "# dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "train_iter = YelpReviewPolarity(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=16, shuffle=False)\n",
        "\n",
        "next(iter(dataloader))\n",
        "#displays 8 labels as batch size = 8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 1]),\n",
              " (\"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\",\n",
              "  \"Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn't judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life.\",\n",
              "  \"I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. I was going to Dr. Johnson before he left and Goldberg took over when Johnson left. He is not a caring doctor. He is only interested in the co-pay and having you come in for medication refills every month. He will not give refills and could less about patients's financial situations. Trying to get your 90 days mail away pharmacy prescriptions through this guy is a joke. And to make matters even worse, his office staff is incompetent. 90% of the time when you call the office, they'll put you through to a voice mail, that NO ONE ever answers or returns your call. Both my adult children and husband have decided to leave this practice after experiencing such frustration. The entire office has an attitude like they are doing you a favor. Give me a break! Stay away from this doc and the practice. You deserve better and they will not be there when you really need them. I have never felt compelled to write a bad review about anyone until I met this pathetic excuse for a doctor who is all about the money.\",\n",
              "  \"I'm writing this review to give you a heads up before you see this Doctor. The office staff and administration are very unprofessional. I left a message with multiple people regarding my bill, and no one ever called me back. I had to hound them to get an answer about my bill. \\\\n\\\\nSecond, and most important, make sure your insurance is going to cover Dr. Goldberg's visits and blood work. He recommended to me that I get a physical, and he knew I was a student because I told him. I got the physical done. Later, I found out my health insurance doesn't pay for preventative visits. I received an $800.00 bill for the blood work. I can't pay for my bill because I'm a student and don't have any cash flow at this current time. I can't believe the Doctor wouldn't give me a heads up to make sure my insurance would cover work that wasn't necessary and was strictly preventative. The office can't do anything to help me cover the bill. In addition, the office staff said the onus is on me to make sure my insurance covers visits. Frustrating situation!\",\n",
              "  'All the food is great here. But the best thing they have is their wings. Their wings are simply fantastic!!  The \\\\\"Wet Cajun\\\\\" are by the best & most popular.  I also like the seasoned salt wings.  Wing Night is Monday & Wednesday night, $0.75 whole wings!\\\\n\\\\nThe dining area is nice. Very family friendly! The bar is very nice is well.  This place is truly a Yinzer\\'s dream!!  \\\\\"Pittsburgh Dad\\\\\" would love this place n\\'at!!',\n",
              "  'Wing sauce is like water. Pretty much a lot of butter and some hot sauce (franks red hot maybe).  The whole wings are good size and crispy, but for $1 a wing the sauce could be better. The hot and extra hot are about the same flavor/heat.  The fish sandwich is good and is a large portion, sides are decent.',\n",
              "  \"Owning a driving range inside the city limits is like a license to print money.  I don't think I ask much out of a driving range.  Decent mats, clean balls and accessible hours.  Hell you need even less people now with the advent of the machine that doles out the balls.  This place has none of them.  It is april and there are no grass tees yet.  BTW they opened for the season this week although it has been golfing weather for a month.  The mats look like the carpet at my 107 year old aunt Irene's house.  Worn and thread bare.  Let's talk about the hours.  This place is equipped with lights yet they only sell buckets of balls until 730.  It is still light out.  Finally lets you have the pit to hit into.  When I arrived I wasn't sure if this was a driving range or an excavation site for a mastodon or a strip mining operation.  There is no grass on the range. Just mud.  Makes it a good tool to figure out how far you actually are hitting the ball.  Oh, they are cash only also.\\\\n\\\\nBottom line, this place sucks.  The best hope is that the owner sells it to someone that actually wants to make money and service golfers in Pittsburgh.\",\n",
              "  \"This place is absolute garbage...  Half of the tees are not available, including all the grass tees.  It is cash only, and they sell the last bucket at 8, despite having lights.  And if you finish even a minute after 8, don't plan on getting a drink.  The vending machines are sold out (of course) and they sell drinks inside, but close the drawers at 8 on the dot.  There are weeds grown all over the place.  I noticed some sort of batting cage, but it looks like those are out of order as well.  Someone should buy this place and turn it into what it should be.\",\n",
              "  \"Before I finally made it over to this range I heard the same thing from most people - it's just fine to go work on your swing. I had such a low expectation I was pleasantly surprised. \\\\n\\\\nIt's a fairly big range - if you are familiar with Scally's in Moon, it seems like it has almost as many tees, though its not nearly as nice a facility. \\\\n\\\\nThe guys in the pro shop were two of the friendlier guys I've come across at ranges or at courses. Yards were indeed marked and there are some targets to aim for, and even some hazards to aim away from. \\\\n\\\\nA big red flag to me was the extra charge ($3) to hit off the grass. I am no range expert, but this is the 4th one I've been to and the first I've seen of that sort of nickel and diming....\\\\n\\\\nPrice for the golf balls was reasonable and I do plan to be back every week until they close up in October for the season. Hopefully, since its for sale, it will reopen as a golf facility again.\",\n",
              "  \"I drove by yesterday to get a sneak peak.  It re-opens on July 14th and I can't wait to take my kids.  The new range looks amazing.  The entire range appears to be turf, which may or many not help your game, but it looks really nice.  The tee boxes look state of the art and the club house looks like something you'll see on a newer course.  Can't wait to experience it!\",\n",
              "  \"After waiting for almost 30 minutes to trade in an old phone part of the buy back program, our customer service rep incorrectly processed the transaction. This led to us waiting another 30 minutes for him to correct it. Don't visit this store if you want pleasant or good service.\",\n",
              "  \"Wonderful reuben.  Map shown on Yelp page is incorrect. It is actually a different Hawkins.  I'd recommend a call for directions 412-271-9911.\",\n",
              "  \"After a morning of Thrift Store hunting, a friend and I were thinking of lunch, and he suggested Emil's after he'd seen Chris Sebak do a bit on it and had tried it a time or two before, and I had not. He said they had a decent Reuben, but to be prepared to step back in time.\\\\n\\\\nWell, seeing as how I'm kind of addicted to late 40's and early 50's, and the whole Rat Pack scene, stepping back in time is a welcomed change in da burgh...as long as it doesn't involve 1979, which I can see all around me every day.\\\\n\\\\nAnd yet another shot at finding a decent Reuben in da burgh...well, that's like hunting the Holy Grail. So looking under one more bush certainly wouldn't hurt.\\\\n\\\\nSo off we go right at lunchtime in the middle of...where exactly were we? At first I thought we were lost, driving around a handful of very rather dismal looking blocks in what looked like a neighborhood that had been blighted by the building of a highway. And then...AHA! Here it is! And yep, there it was. This little unassuming building with an add-on entrance with what looked like a very old hand painted sign stating quite simply 'Emil's. \\\\n\\\\nWe walked in the front door, and entered another world. Another time, and another place. Oh, and any Big Burrito/Sousa foodies might as well stop reading now. I wouldn't want to see you walk in, roll your eyes and say 'Reaaaaaalllly?'\\\\n\\\\nThis is about as old world bar/lounge/restaurant as it gets. Plain, with a dark wood bar on one side, plain white walls with no yinzer pics, good sturdy chairs and actual white linens on the tables. This is the kind of neighborhood dive that I could see Frank and Dino pulling a few tables together for some poker, a fish sammich, and some cheap scotch. And THAT is exactly what I love.\\\\n\\\\nOh...but good food counts too. \\\\n\\\\nWe each had a Reuben, and my friend had a side of fries. The Reubens were decent, but not NY awesome. A little too thick on the bread, but overall, tasty and definitely filling. Not too skimpy on the meat. I seriously CRAVE a true, good NY Reuben, but since I can't afford to travel right now, what I find in da burgh will have to do. But as we sat and ate, burgers came out to an adjoining table. Those were some big thick burgers. A steak went past for the table behind us. That was HUGE! And when we asked about it, the waitress said 'Yeah, it's huge and really good, and he only charges $12.99 for it, ain't that nuts?' Another table of five came in, and wham. Fish sandwiches PILED with breaded fish that looked amazing. Yeah, I want that, that, that and THAT!\\\\n\\\\nMy friend also mentioned that they have a Chicken Parm special one day of the week that is only served UNTIL 4 pm, and that it is fantastic. If only I could GET there on that week day before 4...\\\\n\\\\nThe waitress did a good job, especially since there was quite a growing crowd at lunchtime on a Saturday, and only one of her. She kept up and was very friendly. \\\\n\\\\nThey only have Pepsi products, so I had a brewed iced tea, which was very fresh, and she did pop by to ask about refills as often as she could. As the lunch hour went on, they were getting busy.\\\\n\\\\nEmil's is no frills, good portions, very reasonable prices, VERY comfortable neighborhood hole in the wall...kind of like Cheers, but in a blue collar neighborhood in the 1950's. Fan-freakin-tastic! I could feel at home here.\\\\n\\\\nYou definitely want to hit Mapquest or plug in your GPS though. I am not sure that I could find it again on my own...it really is a hidden gem. I will be making my friend take me back until I can memorize where the heck it is.\\\\n\\\\nAddendum: 2nd visit for the fish sandwich. Excellent. Truly. A pound of fish on a fish-shaped bun (as opposed to da burgh's seemingly popular hamburger bun). The fish was flavorful, the batter excellent, and for just $8. This may have been the best fish sandwich I've yet to have in da burgh.\",\n",
              "  \"This is a hidden gem, no really. It took us forever to find but well worth it. It is right across the street from the Rankin Police Station. The menu has a wide selection, I really couldn't decide what I wanted but I went with the ribeye sandwich. I'm glad i did too. Huge sandwich! I added mushrooms, it was very flavorful. My boyfriend got the fish sandwich, he enjoyed it as well. Fast and friendly service. Will definitely be back.\",\n",
              "  'Awesome drink specials during happy hour. Fantastic wings that are crispy and delicious, wing night on Tuesday and Thursday!',\n",
              "  \"Very disappointed in the customer service. We ordered Reuben's  and wanted coleslaw instead of kraut. They charged us $3.00 for the coleslaw. We will not be back . The iced tea is also terrible tasting.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruhdtQ7JeUDK",
        "outputId": "17d4a267-1c35-43ed-ee39-8778cbe1b679"
      },
      "source": [
        "help(DataLoader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class DataLoader in module torch.utils.data.dataloader:\n",
            "\n",
            "class DataLoader(typing.Generic)\n",
            " |  DataLoader(*args, **kwds)\n",
            " |  \n",
            " |  Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
            " |  the given dataset.\n",
            " |  \n",
            " |  The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
            " |  iterable-style datasets with single- or multi-process loading, customizing\n",
            " |  loading order and optional automatic batching (collation) and memory pinning.\n",
            " |  \n",
            " |  See :py:mod:`torch.utils.data` documentation page for more details.\n",
            " |  \n",
            " |  Args:\n",
            " |      dataset (Dataset): dataset from which to load the data.\n",
            " |      batch_size (int, optional): how many samples per batch to load\n",
            " |          (default: ``1``).\n",
            " |      shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
            " |          at every epoch (default: ``False``).\n",
            " |      sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
            " |          samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
            " |          implemented. If specified, :attr:`shuffle` must not be specified.\n",
            " |      batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
            " |          returns a batch of indices at a time. Mutually exclusive with\n",
            " |          :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
            " |          and :attr:`drop_last`.\n",
            " |      num_workers (int, optional): how many subprocesses to use for data\n",
            " |          loading. ``0`` means that the data will be loaded in the main process.\n",
            " |          (default: ``0``)\n",
            " |      collate_fn (callable, optional): merges a list of samples to form a\n",
            " |          mini-batch of Tensor(s).  Used when using batched loading from a\n",
            " |          map-style dataset.\n",
            " |      pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
            " |          into CUDA pinned memory before returning them.  If your data elements\n",
            " |          are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
            " |          see the example below.\n",
            " |      drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
            " |          if the dataset size is not divisible by the batch size. If ``False`` and\n",
            " |          the size of dataset is not divisible by the batch size, then the last batch\n",
            " |          will be smaller. (default: ``False``)\n",
            " |      timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
            " |          from workers. Should always be non-negative. (default: ``0``)\n",
            " |      worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
            " |          worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
            " |          input, after seeding and before data loading. (default: ``None``)\n",
            " |      generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
            " |          by RandomSampler to generate random indexes and multiprocessing to generate\n",
            " |          `base_seed` for workers. (default: ``None``)\n",
            " |      prefetch_factor (int, optional, keyword-only arg): Number of samples loaded\n",
            " |          in advance by each worker. ``2`` means there will be a total of\n",
            " |          2 * num_workers samples prefetched across all workers. (default: ``2``)\n",
            " |      persistent_workers (bool, optional): If ``True``, the data loader will not shutdown\n",
            " |          the worker processes after a dataset has been consumed once. This allows to\n",
            " |          maintain the workers `Dataset` instances alive. (default: ``False``)\n",
            " |  \n",
            " |  \n",
            " |  .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
            " |               cannot be an unpicklable object, e.g., a lambda function. See\n",
            " |               :ref:`multiprocessing-best-practices` on more details related\n",
            " |               to multiprocessing in PyTorch.\n",
            " |  \n",
            " |  .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
            " |               When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
            " |               it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
            " |               rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
            " |               configurations. This represents the best guess PyTorch can make because PyTorch\n",
            " |               trusts user :attr:`dataset` code in correctly handling multi-process\n",
            " |               loading to avoid duplicate data.\n",
            " |  \n",
            " |               However, if sharding results in multiple workers having incomplete last batches,\n",
            " |               this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
            " |               be broken into multiple ones and (2) more than one batch worth of samples can be\n",
            " |               dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
            " |               cases in general.\n",
            " |  \n",
            " |               See `Dataset Types`_ for more details on these two types of datasets and how\n",
            " |               :class:`~torch.utils.data.IterableDataset` interacts with\n",
            " |               `Multi-process data loading`_.\n",
            " |  \n",
            " |  .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
            " |               :ref:`data-loading-randomness` notes for random seed related questions.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DataLoader\n",
            " |      typing.Generic\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, dataset: torch.utils.data.dataset.Dataset[+T_co], batch_size: Union[int, NoneType] = 1, shuffle: bool = False, sampler: Union[torch.utils.data.sampler.Sampler, NoneType] = None, batch_sampler: Union[torch.utils.data.sampler.Sampler[Sequence], NoneType] = None, num_workers: int = 0, collate_fn: Union[Callable[[List[~T]], Any], NoneType] = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: Union[Callable[[int], NoneType], NoneType] = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self) -> '_BaseDataLoaderIter'\n",
            " |      # We quote '_BaseDataLoaderIter' since it isn't defined yet and the definition can't be moved up\n",
            " |      # since '_BaseDataLoaderIter' references 'DataLoader'.\n",
            " |  \n",
            " |  __len__(self) -> int\n",
            " |  \n",
            " |  __setattr__(self, attr, val)\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  check_worker_number_rationality(self)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  multiprocessing_context\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {'_iterator': typing.Union[ForwardRef('_BaseDataLoad...\n",
            " |  \n",
            " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
            " |  \n",
            " |  __parameters__ = (+T_co,)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __class_getitem__(params) from builtins.type\n",
            " |  \n",
            " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
            " |      This method is called when a class is subclassed.\n",
            " |      \n",
            " |      The default implementation does nothing. It may be\n",
            " |      overridden to extend subclasses.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwds)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OGbN_YIyHfG"
      },
      "source": [
        "Torchtext has revamped the very basic components of the torchtext library, including vocab, word vectors, tokenizer. These are the basic data processing building blocks for a raw text strings. \n",
        "\n",
        "Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Here we use built in factory function build_vocab_from_iterator which accepts iterator that yield list or iterator of tokens. Users can also pass any special symbols to be added to the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ITqjdsvxrnB"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = YelpReviewPolarity(split='train')\n",
        "\n",
        "#to feed in sentences one by one\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "\n",
        "#sets the default index = 0 for the unknown words, i.e., words not in the vocabulary\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWej-eJ7R70m"
      },
      "source": [
        "def st(*transforms):\n",
        "  def func(txt_io):\n",
        "    for transform in transforms:\n",
        "      txt_io = transform(txt_io)\n",
        "    return txt_io\n",
        "  return func"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKhLvaKLzQek",
        "outputId": "a930bc98-e225-4395-fdd7-049f7909cbb1"
      },
      "source": [
        "#theschoolofai is the unknown word in our vocab, hence index set to 0\n",
        "vocab(['here', 'is', 'an', 'example', 'theschoolofai'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[51, 14, 68, 2030, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEg8TR080r_-"
      },
      "source": [
        "Prepare the text processing pipeline with the tokenizer and vocabulary. The text and label pipelines will be used to process the raw data strings from the dataset iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPIlKV9P0gi3"
      },
      "source": [
        "#we need to pass our sentences through the tokenizer first to normalise it before sending it the vocab\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x)-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1fpaYln0zZH",
        "outputId": "a97a69cb-6d25-4313-e32b-716e70cdd2a0"
      },
      "source": [
        "text_pipeline('here is the an example')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[51, 14, 2, 68, 2030]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLAt2Kyi01cZ",
        "outputId": "e179dde8-4704-4024-ea7c-807e3d233764"
      },
      "source": [
        "label_pipeline('10')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcLXuyuD1T4X"
      },
      "source": [
        "## Generate data batch and iterator\n",
        "\n",
        "torch.utils.data.DataLoader is recommended for PyTorch users. It works with a map-style dataset that implements the getitem() and len() protocols, and represents a map from indices/keys to data samples. It also works with an iterable dataset with the shuffle argument of False.\n",
        "\n",
        "Before sending to the model, collate_fn function works on a batch of samples generated from DataLoader. The input to collate_fn is a batch of data with the batch size in DataLoader, and collate_fn processes them according to the data processing pipelines declared previously. Pay attention here and make sure that collate_fn is declared as a top level def. This ensures that the function is available in each worker."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbhW4lz606fh"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpays4p95I_z"
      },
      "source": [
        "EmbeddingBag is a useful feature to consume sparse ids and produce embeddings. Later we are going to see examples where the text entries in the original batch input will be backed into a list and concatenated as a single tensor for the input of nn.EmbeddingBag.\n",
        "\n",
        "It is doing two things. The first step is to create an embedding and the second step is to reduce (sum/mean/max, according to the \"mode\" argument) the embedding output across dimension 0. So this is equivalent to torch.nn.functional.embedding, followed by torch.sum/mean/max. However, the conceptual two step process does not reflect how it's actually implemented. Since embedding_bag does not need to return the intermediate result, it doesn't actually generate a Tensor object for the embedding. It just goes straight to computing the reduction, pulling in the appropriate data from the weight argument according to the indices in the input argument. Avoiding the creation of the embedding Tensor allows for better performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmBi1Nlb2lXr",
        "outputId": "cde7a4a5-78f7-4c39-a1b9-5a7d30795162"
      },
      "source": [
        "weight = torch.randn(3, 4)\n",
        "weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1543, -1.5977, -0.2257,  0.1658],\n",
              "        [-0.3355, -2.6362,  0.4767, -0.1704],\n",
              "        [ 2.6389,  0.8871, -1.5911,  0.5172]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1OmVDGp5N_f",
        "outputId": "e555717b-ec0c-4d7b-f913-b98b7e9cdafc"
      },
      "source": [
        "indices = torch.tensor([2, 1])\n",
        "indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmI3jesm5UEh",
        "outputId": "dc8eae99-3c37-4239-94af-14c7863777e6"
      },
      "source": [
        "embedding_dwork = torch.nn.functional.embedding(indices, weight)\n",
        "embedding_dwork"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.6389,  0.8871, -1.5911,  0.5172],\n",
              "        [-0.3355, -2.6362,  0.4767, -0.1704]])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLPLNGmj5ehJ",
        "outputId": "1c3f8359-ea12-4d9d-83d7-d738a1ba63ba"
      },
      "source": [
        "embedding_dwork_mean = embedding_dwork.mean(dim=0, keepdim=True)\n",
        "embedding_dwork_mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.1517, -0.8746, -0.5572,  0.1734]])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86xXxo575mlx",
        "outputId": "75a92117-ee83-4443-86f0-43eca7ec9210"
      },
      "source": [
        "embedding_bag = torch.nn.functional.embedding_bag(indices, weight, torch.tensor([0]), mode='mean')\n",
        "embedding_bag"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.1517, -0.8746, -0.5572,  0.1734]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG9ExKRG6z2M"
      },
      "source": [
        "In this example, the text entries in the original data batch input are packed into a list and concatenated as a single tensor for the input of nn.EmbeddingBag. The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fo_Yp1I5tQ_"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label)) #appending the labels to label_list\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text) \n",
        "         offsets.append(processed_text.size(0)) #appending the length of the sentence\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQYlLbzq629a"
      },
      "source": [
        "train_iter = YelpReviewPolarity(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=16, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy1IdVAv7Xod"
      },
      "source": [
        "The model is composed of the nn.EmbeddingBag <https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag>__ layer plus a linear layer for the classification purpose. nn.EmbeddingBag with the default mode of \"mean\" computes the mean value of a “bag” of embeddings. Although the text entries here have different lengths, nn.EmbeddingBag module requires no padding here since the text lengths are saved in offsets.\n",
        "\n",
        "Additionally, since nn.EmbeddingBag accumulates the average across the embeddings on the fly, nn.EmbeddingBag can enhance the performance and memory efficiency to process a sequence of tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWiF5WDj7VVK"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQYxfdZc7eWm"
      },
      "source": [
        "We build a model with the embedding dimension of 64. The vocab size is equal to the length of the vocabulary instance. The number of classes is equal to the number of labels,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WCp2e_C7Z5S"
      },
      "source": [
        "train_iter = YelpReviewPolarity(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRLyvS-f7f-D"
      },
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtzEbdi17rDd"
      },
      "source": [
        "Split the dataset and run the model\n",
        "Since the original AG_NEWS has no valid dataset, we split the training dataset into train/valid sets with a split ratio of 0.95 (train) and 0.05 (valid). Here we use torch.utils.data.dataset.random_split <https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split>__ function in PyTorch core library.\n",
        "\n",
        "CrossEntropyLoss <https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>__ criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class. It is useful when training a classification problem with C classes. SGD <https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html>__ implements stochastic gradient descent method as the optimizer. The initial learning rate is set to 5.0. StepLR <https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR>__ is used here to adjust the learning rate through epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkoEyB2b7itr",
        "outputId": "6e0e0fe8-8e0a-4593-f364-a438a2f115e1"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = YelpReviewPolarity()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 8313 batches | accuracy    0.772\n",
            "| epoch   1 |  1000/ 8313 batches | accuracy    0.864\n",
            "| epoch   1 |  1500/ 8313 batches | accuracy    0.881\n",
            "| epoch   1 |  2000/ 8313 batches | accuracy    0.888\n",
            "| epoch   1 |  2500/ 8313 batches | accuracy    0.892\n",
            "| epoch   1 |  3000/ 8313 batches | accuracy    0.897\n",
            "| epoch   1 |  3500/ 8313 batches | accuracy    0.901\n",
            "| epoch   1 |  4000/ 8313 batches | accuracy    0.901\n",
            "| epoch   1 |  4500/ 8313 batches | accuracy    0.905\n",
            "| epoch   1 |  5000/ 8313 batches | accuracy    0.902\n",
            "| epoch   1 |  5500/ 8313 batches | accuracy    0.908\n",
            "| epoch   1 |  6000/ 8313 batches | accuracy    0.911\n",
            "| epoch   1 |  6500/ 8313 batches | accuracy    0.912\n",
            "| epoch   1 |  7000/ 8313 batches | accuracy    0.911\n",
            "| epoch   1 |  7500/ 8313 batches | accuracy    0.910\n",
            "| epoch   1 |  8000/ 8313 batches | accuracy    0.911\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 166.20s | valid accuracy    0.919 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 8313 batches | accuracy    0.910\n",
            "| epoch   2 |  1000/ 8313 batches | accuracy    0.915\n",
            "| epoch   2 |  1500/ 8313 batches | accuracy    0.916\n",
            "| epoch   2 |  2000/ 8313 batches | accuracy    0.916\n",
            "| epoch   2 |  2500/ 8313 batches | accuracy    0.917\n",
            "| epoch   2 |  3000/ 8313 batches | accuracy    0.915\n",
            "| epoch   2 |  3500/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  4000/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  4500/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  5000/ 8313 batches | accuracy    0.914\n",
            "| epoch   2 |  5500/ 8313 batches | accuracy    0.919\n",
            "| epoch   2 |  6000/ 8313 batches | accuracy    0.921\n",
            "| epoch   2 |  6500/ 8313 batches | accuracy    0.921\n",
            "| epoch   2 |  7000/ 8313 batches | accuracy    0.921\n",
            "| epoch   2 |  7500/ 8313 batches | accuracy    0.920\n",
            "| epoch   2 |  8000/ 8313 batches | accuracy    0.919\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 164.69s | valid accuracy    0.925 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 8313 batches | accuracy    0.918\n",
            "| epoch   3 |  1000/ 8313 batches | accuracy    0.921\n",
            "| epoch   3 |  1500/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  2000/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  2500/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  3000/ 8313 batches | accuracy    0.921\n",
            "| epoch   3 |  3500/ 8313 batches | accuracy    0.923\n",
            "| epoch   3 |  4000/ 8313 batches | accuracy    0.923\n",
            "| epoch   3 |  4500/ 8313 batches | accuracy    0.924\n",
            "| epoch   3 |  5000/ 8313 batches | accuracy    0.921\n",
            "| epoch   3 |  5500/ 8313 batches | accuracy    0.924\n",
            "| epoch   3 |  6000/ 8313 batches | accuracy    0.925\n",
            "| epoch   3 |  6500/ 8313 batches | accuracy    0.926\n",
            "| epoch   3 |  7000/ 8313 batches | accuracy    0.925\n",
            "| epoch   3 |  7500/ 8313 batches | accuracy    0.924\n",
            "| epoch   3 |  8000/ 8313 batches | accuracy    0.924\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 166.51s | valid accuracy    0.926 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 8313 batches | accuracy    0.921\n",
            "| epoch   4 |  1000/ 8313 batches | accuracy    0.925\n",
            "| epoch   4 |  1500/ 8313 batches | accuracy    0.924\n",
            "| epoch   4 |  2000/ 8313 batches | accuracy    0.925\n",
            "| epoch   4 |  2500/ 8313 batches | accuracy    0.925\n",
            "| epoch   4 |  3000/ 8313 batches | accuracy    0.925\n",
            "| epoch   4 |  3500/ 8313 batches | accuracy    0.926\n",
            "| epoch   4 |  4000/ 8313 batches | accuracy    0.927\n",
            "| epoch   4 |  4500/ 8313 batches | accuracy    0.928\n",
            "| epoch   4 |  5000/ 8313 batches | accuracy    0.924\n",
            "| epoch   4 |  5500/ 8313 batches | accuracy    0.927\n",
            "| epoch   4 |  6000/ 8313 batches | accuracy    0.928\n",
            "| epoch   4 |  6500/ 8313 batches | accuracy    0.928\n",
            "| epoch   4 |  7000/ 8313 batches | accuracy    0.927\n",
            "| epoch   4 |  7500/ 8313 batches | accuracy    0.926\n",
            "| epoch   4 |  8000/ 8313 batches | accuracy    0.926\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 163.29s | valid accuracy    0.927 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 8313 batches | accuracy    0.924\n",
            "| epoch   5 |  1000/ 8313 batches | accuracy    0.927\n",
            "| epoch   5 |  1500/ 8313 batches | accuracy    0.927\n",
            "| epoch   5 |  2000/ 8313 batches | accuracy    0.928\n",
            "| epoch   5 |  2500/ 8313 batches | accuracy    0.928\n",
            "| epoch   5 |  3000/ 8313 batches | accuracy    0.927\n",
            "| epoch   5 |  3500/ 8313 batches | accuracy    0.928\n",
            "| epoch   5 |  4000/ 8313 batches | accuracy    0.930\n",
            "| epoch   5 |  4500/ 8313 batches | accuracy    0.930\n",
            "| epoch   5 |  5000/ 8313 batches | accuracy    0.926\n",
            "| epoch   5 |  5500/ 8313 batches | accuracy    0.929\n",
            "| epoch   5 |  6000/ 8313 batches | accuracy    0.930\n",
            "| epoch   5 |  6500/ 8313 batches | accuracy    0.930\n",
            "| epoch   5 |  7000/ 8313 batches | accuracy    0.928\n",
            "| epoch   5 |  7500/ 8313 batches | accuracy    0.928\n",
            "| epoch   5 |  8000/ 8313 batches | accuracy    0.928\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 161.67s | valid accuracy    0.928 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/ 8313 batches | accuracy    0.926\n",
            "| epoch   6 |  1000/ 8313 batches | accuracy    0.929\n",
            "| epoch   6 |  1500/ 8313 batches | accuracy    0.928\n",
            "| epoch   6 |  2000/ 8313 batches | accuracy    0.930\n",
            "| epoch   6 |  2500/ 8313 batches | accuracy    0.929\n",
            "| epoch   6 |  3000/ 8313 batches | accuracy    0.928\n",
            "| epoch   6 |  3500/ 8313 batches | accuracy    0.931\n",
            "| epoch   6 |  4000/ 8313 batches | accuracy    0.931\n",
            "| epoch   6 |  4500/ 8313 batches | accuracy    0.931\n",
            "| epoch   6 |  5000/ 8313 batches | accuracy    0.929\n",
            "| epoch   6 |  5500/ 8313 batches | accuracy    0.931\n",
            "| epoch   6 |  6000/ 8313 batches | accuracy    0.932\n",
            "| epoch   6 |  6500/ 8313 batches | accuracy    0.932\n",
            "| epoch   6 |  7000/ 8313 batches | accuracy    0.930\n",
            "| epoch   6 |  7500/ 8313 batches | accuracy    0.930\n",
            "| epoch   6 |  8000/ 8313 batches | accuracy    0.929\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 164.17s | valid accuracy    0.929 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/ 8313 batches | accuracy    0.927\n",
            "| epoch   7 |  1000/ 8313 batches | accuracy    0.931\n",
            "| epoch   7 |  1500/ 8313 batches | accuracy    0.930\n",
            "| epoch   7 |  2000/ 8313 batches | accuracy    0.932\n",
            "| epoch   7 |  2500/ 8313 batches | accuracy    0.930\n",
            "| epoch   7 |  3000/ 8313 batches | accuracy    0.930\n",
            "| epoch   7 |  3500/ 8313 batches | accuracy    0.932\n",
            "| epoch   7 |  4000/ 8313 batches | accuracy    0.932\n",
            "| epoch   7 |  4500/ 8313 batches | accuracy    0.932\n",
            "| epoch   7 |  5000/ 8313 batches | accuracy    0.930\n",
            "| epoch   7 |  5500/ 8313 batches | accuracy    0.933\n",
            "| epoch   7 |  6000/ 8313 batches | accuracy    0.934\n",
            "| epoch   7 |  6500/ 8313 batches | accuracy    0.933\n",
            "| epoch   7 |  7000/ 8313 batches | accuracy    0.931\n",
            "| epoch   7 |  7500/ 8313 batches | accuracy    0.931\n",
            "| epoch   7 |  8000/ 8313 batches | accuracy    0.931\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 162.22s | valid accuracy    0.930 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/ 8313 batches | accuracy    0.928\n",
            "| epoch   8 |  1000/ 8313 batches | accuracy    0.932\n",
            "| epoch   8 |  1500/ 8313 batches | accuracy    0.931\n",
            "| epoch   8 |  2000/ 8313 batches | accuracy    0.933\n",
            "| epoch   8 |  2500/ 8313 batches | accuracy    0.931\n",
            "| epoch   8 |  3000/ 8313 batches | accuracy    0.931\n",
            "| epoch   8 |  3500/ 8313 batches | accuracy    0.933\n",
            "| epoch   8 |  4000/ 8313 batches | accuracy    0.933\n",
            "| epoch   8 |  4500/ 8313 batches | accuracy    0.933\n",
            "| epoch   8 |  5000/ 8313 batches | accuracy    0.931\n",
            "| epoch   8 |  5500/ 8313 batches | accuracy    0.934\n",
            "| epoch   8 |  6000/ 8313 batches | accuracy    0.935\n",
            "| epoch   8 |  6500/ 8313 batches | accuracy    0.935\n",
            "| epoch   8 |  7000/ 8313 batches | accuracy    0.932\n",
            "| epoch   8 |  7500/ 8313 batches | accuracy    0.932\n",
            "| epoch   8 |  8000/ 8313 batches | accuracy    0.932\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 162.58s | valid accuracy    0.930 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/ 8313 batches | accuracy    0.929\n",
            "| epoch   9 |  1000/ 8313 batches | accuracy    0.933\n",
            "| epoch   9 |  1500/ 8313 batches | accuracy    0.933\n",
            "| epoch   9 |  2000/ 8313 batches | accuracy    0.933\n",
            "| epoch   9 |  2500/ 8313 batches | accuracy    0.932\n",
            "| epoch   9 |  3000/ 8313 batches | accuracy    0.932\n",
            "| epoch   9 |  3500/ 8313 batches | accuracy    0.934\n",
            "| epoch   9 |  4000/ 8313 batches | accuracy    0.934\n",
            "| epoch   9 |  4500/ 8313 batches | accuracy    0.935\n",
            "| epoch   9 |  5000/ 8313 batches | accuracy    0.932\n",
            "| epoch   9 |  5500/ 8313 batches | accuracy    0.935\n",
            "| epoch   9 |  6000/ 8313 batches | accuracy    0.936\n",
            "| epoch   9 |  6500/ 8313 batches | accuracy    0.936\n",
            "| epoch   9 |  7000/ 8313 batches | accuracy    0.933\n",
            "| epoch   9 |  7500/ 8313 batches | accuracy    0.933\n",
            "| epoch   9 |  8000/ 8313 batches | accuracy    0.932\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 161.73s | valid accuracy    0.931 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/ 8313 batches | accuracy    0.930\n",
            "| epoch  10 |  1000/ 8313 batches | accuracy    0.934\n",
            "| epoch  10 |  1500/ 8313 batches | accuracy    0.934\n",
            "| epoch  10 |  2000/ 8313 batches | accuracy    0.934\n",
            "| epoch  10 |  2500/ 8313 batches | accuracy    0.933\n",
            "| epoch  10 |  3000/ 8313 batches | accuracy    0.932\n",
            "| epoch  10 |  3500/ 8313 batches | accuracy    0.934\n",
            "| epoch  10 |  4000/ 8313 batches | accuracy    0.935\n",
            "| epoch  10 |  4500/ 8313 batches | accuracy    0.935\n",
            "| epoch  10 |  5000/ 8313 batches | accuracy    0.933\n",
            "| epoch  10 |  5500/ 8313 batches | accuracy    0.936\n",
            "| epoch  10 |  6000/ 8313 batches | accuracy    0.936\n",
            "| epoch  10 |  6500/ 8313 batches | accuracy    0.937\n",
            "| epoch  10 |  7000/ 8313 batches | accuracy    0.934\n",
            "| epoch  10 |  7500/ 8313 batches | accuracy    0.934\n",
            "| epoch  10 |  8000/ 8313 batches | accuracy    0.933\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 162.58s | valid accuracy    0.931 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbksWIvf7mQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96cad176-df7f-4f18-d0d4-c86fef84ae2c"
      },
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbyPGkgBRpmF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}