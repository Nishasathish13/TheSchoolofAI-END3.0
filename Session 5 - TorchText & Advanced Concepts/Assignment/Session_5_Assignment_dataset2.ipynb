{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 5-Assignment_dataset2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishasathish13/TheSchoolofAI-END3.0/blob/main/Session%205%20-%20TorchText%20%26%20Advanced%20Concepts/Assignment/Session_5_Assignment_dataset2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VaVIQPawdbU"
      },
      "source": [
        "from torchtext.datasets import IMDB"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIDrrV3AcpNu",
        "outputId": "d11f1301-503e-4e33-c021-15c924c70700"
      },
      "source": [
        "help(IMDB)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function IMDB in module torchtext.datasets.imdb:\n",
            "\n",
            "IMDB(root='.data', split=('train', 'test'))\n",
            "    IMDB dataset\n",
            "    \n",
            "    Separately returns the train/test split\n",
            "    \n",
            "    Number of lines per split:\n",
            "        train: 25000\n",
            "    \n",
            "        test: 25000\n",
            "    \n",
            "    \n",
            "    Number of classes\n",
            "        2\n",
            "    \n",
            "    \n",
            "    Args:\n",
            "        root: Directory where the datasets are saved.\n",
            "            Default: .data\n",
            "        split: split or splits to be returned. Can be a string or tuple of strings.\n",
            "            Default: ('train', 'test')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBvZyNEAwtgf",
        "outputId": "ae37ac33-fb4f-4db3-9db7-37e933e424f6"
      },
      "source": [
        "train_iter = IMDB(split='train')\n",
        "next(train_iter)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('neg',\n",
              " 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrC7Ywp0w2j9"
      },
      "source": [
        "# or iterate over a loop\n",
        "\n",
        "#to display only the first 20 lines\n",
        "for (ln, (label, line)) in enumerate(train_iter):\n",
        "  print(ln, label, line)\n",
        "  if ln == 20:\n",
        "    break\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDP3CqFKxZ7X"
      },
      "source": [
        "# dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "train_iter = IMDB(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=16, shuffle=False)\n",
        "\n",
        "next(iter(dataloader))\n",
        "#displays 8 labels as batch size = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting the labels \"pos\", \"neg\" to 0 and 1 respectively\n",
        "\n",
        "labellist = ['pos', 'neg']\n",
        "label_dict = {}\n",
        "for idx, item in enumerate(labellist):\n",
        "  label_dict[item]=idx\n",
        "\n",
        "label = label_dict['neg']\n",
        "label"
      ],
      "metadata": {
        "id": "OCOIwq8_E5CI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruhdtQ7JeUDK"
      },
      "source": [
        "help(DataLoader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OGbN_YIyHfG"
      },
      "source": [
        "Torchtext has revamped the very basic components of the torchtext library, including vocab, word vectors, tokenizer. These are the basic data processing building blocks for a raw text strings. \n",
        "\n",
        "Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Here we use built in factory function build_vocab_from_iterator which accepts iterator that yield list or iterator of tokens. Users can also pass any special symbols to be added to the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ITqjdsvxrnB"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = IMDB(split='train')\n",
        "\n",
        "#to feed in sentences one by one\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "\n",
        "#sets the default index = 0 for the unknown words, i.e., words not in the vocabulary\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWej-eJ7R70m"
      },
      "source": [
        "def st(*transforms):\n",
        "  def func(txt_io):\n",
        "    for transform in transforms:\n",
        "      txt_io = transform(txt_io)\n",
        "    return txt_io\n",
        "  return func"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKhLvaKLzQek"
      },
      "source": [
        "#theschoolofai is the unknown word in our vocab, hence index set to 0\n",
        "#vocab(['here', 'is', 'an', 'example', 'theschoolofai'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEg8TR080r_-"
      },
      "source": [
        "Prepare the text processing pipeline with the tokenizer and vocabulary. The text and label pipelines will be used to process the raw data strings from the dataset iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPIlKV9P0gi3"
      },
      "source": [
        "#we need to pass our sentences through the tokenizer first to normalise it before sending it the vocab\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1fpaYln0zZH"
      },
      "source": [
        "#text_pipeline('here is the an example')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLAt2Kyi01cZ",
        "outputId": "f491b636-c588-4728-d711-3bf9e87e097d"
      },
      "source": [
        "label_pipeline('1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcLXuyuD1T4X"
      },
      "source": [
        "## Generate data batch and iterator\n",
        "\n",
        "torch.utils.data.DataLoader is recommended for PyTorch users. It works with a map-style dataset that implements the getitem() and len() protocols, and represents a map from indices/keys to data samples. It also works with an iterable dataset with the shuffle argument of False.\n",
        "\n",
        "Before sending to the model, collate_fn function works on a batch of samples generated from DataLoader. The input to collate_fn is a batch of data with the batch size in DataLoader, and collate_fn processes them according to the data processing pipelines declared previously. Pay attention here and make sure that collate_fn is declared as a top level def. This ensures that the function is available in each worker."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbhW4lz606fh"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpays4p95I_z"
      },
      "source": [
        "EmbeddingBag is a useful feature to consume sparse ids and produce embeddings. Later we are going to see examples where the text entries in the original batch input will be backed into a list and concatenated as a single tensor for the input of nn.EmbeddingBag.\n",
        "\n",
        "It is doing two things. The first step is to create an embedding and the second step is to reduce (sum/mean/max, according to the \"mode\" argument) the embedding output across dimension 0. So this is equivalent to torch.nn.functional.embedding, followed by torch.sum/mean/max. However, the conceptual two step process does not reflect how it's actually implemented. Since embedding_bag does not need to return the intermediate result, it doesn't actually generate a Tensor object for the embedding. It just goes straight to computing the reduction, pulling in the appropriate data from the weight argument according to the indices in the input argument. Avoiding the creation of the embedding Tensor allows for better performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmBi1Nlb2lXr"
      },
      "source": [
        "weight = torch.randn(3, 4)\n",
        "weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1OmVDGp5N_f"
      },
      "source": [
        "indices = torch.tensor([2, 1])\n",
        "indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmI3jesm5UEh"
      },
      "source": [
        "embedding_dwork = torch.nn.functional.embedding(indices, weight)\n",
        "embedding_dwork"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLPLNGmj5ehJ"
      },
      "source": [
        "embedding_dwork_mean = embedding_dwork.mean(dim=0, keepdim=True)\n",
        "embedding_dwork_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86xXxo575mlx"
      },
      "source": [
        "embedding_bag = torch.nn.functional.embedding_bag(indices, weight, torch.tensor([0]), mode='mean')\n",
        "embedding_bag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG9ExKRG6z2M"
      },
      "source": [
        "In this example, the text entries in the original data batch input are packed into a list and concatenated as a single tensor for the input of nn.EmbeddingBag. The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fo_Yp1I5tQ_"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "        #print(_label)\n",
        "        #label_list.append(label_pipeline(_label)) #appending the labels to label_list\n",
        "        label_list.append(label_dict[_label]) #appending the converted labels to label_list\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text) \n",
        "        offsets.append(processed_text.size(0)) #appending the length of the sentence\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)    "
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQYlLbzq629a"
      },
      "source": [
        "train_iter = IMDB(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=16, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy1IdVAv7Xod"
      },
      "source": [
        "The model is composed of the nn.EmbeddingBag <https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag>__ layer plus a linear layer for the classification purpose. nn.EmbeddingBag with the default mode of \"mean\" computes the mean value of a “bag” of embeddings. Although the text entries here have different lengths, nn.EmbeddingBag module requires no padding here since the text lengths are saved in offsets.\n",
        "\n",
        "Additionally, since nn.EmbeddingBag accumulates the average across the embeddings on the fly, nn.EmbeddingBag can enhance the performance and memory efficiency to process a sequence of tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWiF5WDj7VVK"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQYxfdZc7eWm"
      },
      "source": [
        "We build a model with the embedding dimension of 64. The vocab size is equal to the length of the vocabulary instance. The number of classes is equal to the number of labels,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WCp2e_C7Z5S"
      },
      "source": [
        "train_iter = IMDB(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRLyvS-f7f-D"
      },
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        #print(label)\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        #loss = criterion(predited_label, label)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtzEbdi17rDd"
      },
      "source": [
        "Split the dataset and run the model\n",
        "we split the training dataset into train/valid sets with a split ratio of 0.95 (train) and 0.05 (valid). Here we use torch.utils.data.dataset.random_split <https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split>__ function in PyTorch core library.\n",
        "\n",
        "CrossEntropyLoss <https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>__ criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class. It is useful when training a classification problem with C classes. SGD <https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html>__ implements stochastic gradient descent method as the optimizer. The initial learning rate is set to 5.0. StepLR <https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR>__ is used here to adjust the learning rate through epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkoEyB2b7itr",
        "outputId": "fe33edfd-dcb4-4e05-b0f4-a32b0986a2c3"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = IMDB()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 10.84s | valid accuracy    0.758 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 10.91s | valid accuracy    0.762 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 10.91s | valid accuracy    0.739 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 10.87s | valid accuracy    0.857 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 10.94s | valid accuracy    0.860 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 10.90s | valid accuracy    0.862 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 10.92s | valid accuracy    0.864 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 10.87s | valid accuracy    0.863 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 10.90s | valid accuracy    0.870 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 10.96s | valid accuracy    0.869 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbksWIvf7mQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3bb7d29-cb41-44d0-9275-11951e3e792f"
      },
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y6OjztL8RU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf9c18a4-ea86-40e4-a175-788e3c84ac86"
      },
      "source": [
        "help(get_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function get_tokenizer in module torchtext.data.utils:\n",
            "\n",
            "get_tokenizer(tokenizer, language='en')\n",
            "    Generate tokenizer function for a string sentence.\n",
            "    \n",
            "    Args:\n",
            "        tokenizer: the name of tokenizer function. If None, it returns split()\n",
            "            function, which splits the string sentence by space.\n",
            "            If basic_english, it returns _basic_english_normalize() function,\n",
            "            which normalize the string first and split by space. If a callable\n",
            "            function, it will return the function. If a tokenizer library\n",
            "            (e.g. spacy, moses, toktok, revtok, subword), it returns the\n",
            "            corresponding library.\n",
            "        language: Default en\n",
            "    \n",
            "    Examples:\n",
            "        >>> import torchtext\n",
            "        >>> from torchtext.data import get_tokenizer\n",
            "        >>> tokenizer = get_tokenizer(\"basic_english\")\n",
            "        >>> tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
            "        >>> tokens\n",
            "        >>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbyPGkgBRpmF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}